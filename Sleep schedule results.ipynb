{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a231f9",
   "metadata": {},
   "source": [
    "### Sleep simulations\n",
    "\n",
    "#### Installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.24.2\n",
    "!pip tensorflow-macos==2.11.0\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12409883",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86c95a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import logging\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import csv\n",
    "import torch\n",
    "from wonderwords import RandomWord\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import permutations\n",
    "import logging\n",
    "from random import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import math\n",
    "import evaluate\n",
    "from scipy.stats import sem\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b41698-4c4e-4631-a28b-18507f8e25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT:\n",
    "\n",
    "    def __init__(self, base_model):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(base_model)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(base_model)\n",
    "\n",
    "    def continue_input(self, input_sequence, max_length=200, num_return_sequences=1, no_repeat_ngram_size=0,\n",
    "                       do_sample=False, temperature=0.7, num_beams=1):\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(input_sequence, return_tensors='pt')\n",
    "\n",
    "        # Generate text\n",
    "        output = self.model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_beams,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # Decode the output\n",
    "        sequence = output[0].tolist()\n",
    "        text = self.tokenizer.decode(sequence)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ab239d-3277-4efe-9ef6-e05841ff55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_script(name_or_path='spatial_model', \n",
    "                       num_epochs=3,\n",
    "                       output_dir='./clm_script',\n",
    "                       save_steps=100,\n",
    "                       lr=5e-05 ):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    ! python ./run_clm.py \\\n",
    "        --model_name_or_path {name_or_path} \\\n",
    "        --train_file {os.path.join(output_dir, 'train.txt')} \\\n",
    "        --validation_file {os.path.join(output_dir, 'train.txt')} \\\n",
    "        --per_device_train_batch_size 1 \\\n",
    "        --per_device_eval_batch_size 1 \\\n",
    "        --do_train \\\n",
    "        --do_eval \\\n",
    "        --output_dir {output_dir} \\\n",
    "        --overwrite_output_dir \\\n",
    "        --num_train_epochs {num_epochs} \\\n",
    "        --save_strategy 'steps' \\\n",
    "        --save_steps {save_steps} \\\n",
    "        --learning_rate {lr}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb7ab919-c400-4493-a3c7-8886d564b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RandomWord()\n",
    "\n",
    "def create_unique_random_grid(nouns, size=3):\n",
    "    \"\"\"Creates a size x size grid with unique random nouns.\"\"\"\n",
    "    random_nouns = random.sample(nouns, size * size)\n",
    "    return [random_nouns[i * size:(i + 1) * size] for i in range(size)]\n",
    "\n",
    "def find_shortest_paths(grid, start_name, end_name):\n",
    "    \"\"\"Finds all shortest paths from start_name to end_name in a grid. \"\"\"\n",
    "    # Find coordinates of start and end points\n",
    "    start = end = None\n",
    "    for i, row in enumerate(grid):\n",
    "        for j, name in enumerate(row):\n",
    "            if name == start_name:\n",
    "                start = (i, j)\n",
    "            if name == end_name:\n",
    "                end = (i, j)\n",
    "    \n",
    "    # Check if start or end points were not found\n",
    "    if start is None or end is None:\n",
    "        print (\"start or end not found\")\n",
    "        return []\n",
    "\n",
    "    paths = []\n",
    "    start_x, start_y = start\n",
    "    end_x, end_y = end\n",
    "\n",
    "    # Total horizontal and vertical distances\n",
    "    x_dist = end_x - start_x\n",
    "    y_dist = end_y - start_y\n",
    "\n",
    "    # Generate a list of directions taken in the shortest path\n",
    "    # We know that the shortest route is x_dist EAST or WESTs, and y_dist NORTH or SOUTHs\n",
    "    hor_moves = ['EAST' if x_dist > 0 else 'WEST'] * abs(x_dist)\n",
    "    ver_moves = ['SOUTH' if y_dist > 0 else 'NORTH'] * abs(y_dist)\n",
    "    all_moves = hor_moves + ver_moves\n",
    "\n",
    "    # We have a list, e.g. [NORTH, NORTH, EAST, EAST] and we want to find all possible orderings\n",
    "    # Each ordering (i.e. permutation) is a possible shortest path from start_name to end_name\n",
    "    for path in set(permutations(all_moves, len(all_moves))):\n",
    "        sequence = [f'FROM: {start_name}, TO: {end_name}, PATH: {start_name}']\n",
    "        x, y = start\n",
    "        for direction in path:\n",
    "            if direction == 'EAST' and x < 2:\n",
    "                x += 1\n",
    "            elif direction == 'WEST' and x > 0:\n",
    "                x -= 1\n",
    "            elif direction == 'SOUTH' and y < 2:\n",
    "                y += 1\n",
    "            elif direction == 'NORTH' and y > 0:\n",
    "                y -= 1\n",
    "            else:\n",
    "                # Invalid move, skip this path\n",
    "                break\n",
    "            sequence.append(f\"{direction} {grid[x][y]}\")\n",
    "\n",
    "            # add the path when it successfully reaches the end point\n",
    "            if (x, y) == end:\n",
    "                paths.append(' '.join(sequence))\n",
    "\n",
    "    return paths\n",
    "  \n",
    "# # example usage\n",
    "# grid = create_unique_random_grid(nouns)\n",
    "# paths = find_shortest_paths(grid, grid[0][0], grid[2][2])\n",
    "\n",
    "# # print the grid and the paths to see the output\n",
    "# print(\"Grid:\", grid)\n",
    "# print(\"Shortest Paths:\", paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6e3e660-1cd8-4739-9654-63b9abb96965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_stimuli(stimuli):\n",
    "    random.shuffle(stimuli)\n",
    "    return stimuli\n",
    "\n",
    "def get_all_paths_for_grid(grid):\n",
    "    all_paths = []\n",
    "    items = [item for sublist in grid for item in sublist]\n",
    "    for start in items:\n",
    "        for end in items:\n",
    "            if start != end:\n",
    "                all_paths.extend(find_shortest_paths(grid, start, end))\n",
    "    return shuffle_stimuli(all_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8424d28",
   "metadata": {},
   "source": [
    "#### Test generative replay\n",
    "\n",
    "Let's first create training data for 5 environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "721d2d1e-ab29-4f22-935e-fede85bef7e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_on_env(training_strs, testing_strs, env=0, base_model='base_model', generated_strs=[], num_train=1000, eps=20):\n",
    "    # if base_model != f'spatial_model_{env}':\n",
    "    #     !rm -rf spatial_model_{env}\n",
    "    #     !mkdir spatial_model_{env}\n",
    "    \n",
    "    text_file = open(f\"spatial_model_{env}/train.txt\", \"w\")\n",
    "    list_to_write = np.random.choice(training_strs[env], num_train).tolist()\n",
    "    \n",
    "    list_to_write.extend(generated_strs)\n",
    "    shuffle(list_to_write)\n",
    "    \n",
    "    n = text_file.write('\\n'.join(list_to_write))\n",
    "    text_file.close()\n",
    "\n",
    "    text_file = open(f\"spatial_model_{env}/test.txt\", \"w\")\n",
    "    n = text_file.write('\\n'.join(testing_strs[env]))\n",
    "    text_file.close()\n",
    "\n",
    "    print(f\"About to train model from {base_model} and save to spatial_model_{env}\")\n",
    "    \n",
    "    train_model_script(name_or_path=base_model, \n",
    "                       output_dir=f'spatial_model_{env}', \n",
    "                       num_epochs=eps, \n",
    "                       save_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3698e1f8-8c87-4c13-9d74-8d0f869ed4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generative_replay(model, num=100, temperature=1):\n",
    "    examples = []\n",
    "    while len(examples) < num:\n",
    "        out = model.continue_input(\"FROM:\", \n",
    "                                   do_sample=True,\n",
    "                                   temperature=temperature)\n",
    "        examples.extend(out.split('\\n'))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38089487-731c-463c-969b-ec624d37060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, test_data):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    directions = ['NORTH', 'EAST', 'SOUTH', 'WEST']\n",
    "\n",
    "    for sequence in test_data:\n",
    "        # Find the first direction in the PATH and create the input sequence up to that point\n",
    "        first_direction_index = next((i for i, word in enumerate(sequence.split()) if word in directions), None)\n",
    "        if first_direction_index is not None:\n",
    "            # Prepare the input sequence up to and including the first direction\n",
    "            input_sequence = ' '.join(sequence.split()[:first_direction_index + 1])\n",
    "            \n",
    "            # Generate the model's prediction\n",
    "            full_predicted_sequence = model.continue_input(input_sequence)\n",
    "            # Remove the input part from the predicted sequence\n",
    "            predicted_sequence = full_predicted_sequence[len(input_sequence):].strip()\n",
    "            predicted_token = predicted_sequence.split()[0]  # First word of the generation\n",
    "\n",
    "            # Extract the corresponding true token\n",
    "            target_token = sequence.split()[first_direction_index + 1]\n",
    "\n",
    "            # Compare the predicted token with the true token\n",
    "            total_predictions += 1\n",
    "            print(f\"Correct location: {target_token}, Predicted location: {predicted_token}\")\n",
    "            if predicted_token == target_token:\n",
    "                correct_predictions += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "def get_mean_perplexity(input_texts, model_path):\n",
    "    perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "    results = perplexity.compute(model_id=model_path,\n",
    "                                 add_start_token=False,\n",
    "                                 predictions=input_texts)\n",
    "    return results['mean_perplexity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30ad7f23-511e-482e-a444-aec75fbae727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    training_strs = []\n",
    "    testing_strs = []\n",
    "    for i in range(5):\n",
    "        nouns = [r.word(include_parts_of_speech=[\"nouns\"]).replace(\" \", \"_\") for _ in range(9)]\n",
    "        grid = create_unique_random_grid(nouns)\n",
    "        print(grid)\n",
    "        pths = get_all_paths_for_grid(grid)\n",
    "        training_strs.append(pths[0:100])\n",
    "        testing_strs.append(pths[100:])\n",
    "    \n",
    "    for env in range(5):\n",
    "        if os.path.exists(f\"spatial_model_{env}\") is False:\n",
    "            os.mkdir(f\"spatial_model_{env}\")\n",
    "        text_file = open(f\"spatial_model_{env}/test.txt\", \"w\")\n",
    "        n = text_file.write('\\n'.join(testing_strs[env]))\n",
    "        text_file.close()\n",
    "    \n",
    "    return training_strs, testing_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5b8a236-9cbe-4c3e-9988-bbce9b2cd191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_paired_bar(results):\n",
    "    # Convert results to a structured array\n",
    "    structured_data = np.concatenate([np.array(res[0]) for res in results])\n",
    "\n",
    "    # Determine unique test environments\n",
    "    test_envs = np.unique(structured_data[:, 1])\n",
    "\n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    # Process data for each test environment\n",
    "    for i, env in enumerate(test_envs):\n",
    "        # Filter data for each environment and phase\n",
    "        before_data = structured_data[(structured_data[:, 1] == env) & (structured_data[:, 0] == 0)]\n",
    "        after_data = structured_data[(structured_data[:, 1] == env) & (structured_data[:, 0] == 1)]\n",
    "\n",
    "        # Calculate mean and SEM for before and after\n",
    "        means = [np.mean(before_data[:, 2]), np.mean(after_data[:, 2])]\n",
    "        errors = [sem(before_data[:, 2]) if len(before_data[:, 2]) > 1 else 0, \n",
    "                  sem(after_data[:, 2]) if len(after_data[:, 2]) > 1 else 0]\n",
    "\n",
    "        # Create bars for this test environment\n",
    "        ax.bar(np.arange(len(means)) + i * bar_width, means, bar_width, alpha=opacity, color=plt.cm.Paired(i), yerr=errors, label=f'Test Env {int(env)}')\n",
    "\n",
    "    ax.set_xlabel('Training Phase')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy before and after \\'sleep\\'')\n",
    "    ax.set_xticks(np.arange(len(means)) + bar_width / 2)\n",
    "    ax.set_xticklabels(['Before', 'After'])\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def perplexity_plot(results):\n",
    "    combined_perplexity = np.concatenate([np.array(res[1]) for res in results])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    colors = ['blue', 'green']\n",
    "\n",
    "    for j in range(2):\n",
    "        env_perplexity = combined_perplexity[combined_perplexity[:, 1] == j]\n",
    "        cycles = np.unique(env_perplexity[:, 0])\n",
    "\n",
    "        mean_perplexities = [np.mean(env_perplexity[env_perplexity[:, 0] == cycle, 2]) for cycle in cycles]\n",
    "        perplexity_sems = [sem(env_perplexity[env_perplexity[:, 0] == cycle, 2]) if len(env_perplexity[env_perplexity[:, 0] == cycle, 2]) > 1 else 0 for cycle in cycles]\n",
    "        ax.errorbar(cycles, mean_perplexities, yerr=perplexity_sems, label=f'Test: Env {j}', marker='o', color=colors[j])\n",
    "\n",
    "    ax.set_xlabel('Cycle')\n",
    "    ax.set_ylabel('Perplexity')\n",
    "    ax.set_title('Model Perplexity Across Cycles')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71f06049-0f01-4480-9b36-11aebef7db72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['extent', 'pin', 'jet'], ['boom', 'pioneer', 'pilaf'], ['championship', 'signature', 'apartment']]\n",
      "[['dictaphone', 'nucleotidase', 'ovary'], ['grape', 'backburn', 'metabolite'], ['grasshopper', 'technician', 'forum']]\n",
      "[['aglet', 'ptarmigan', 'property'], ['aim', 'seafood', 'dishwasher'], ['anticodon', 'wild', 'wiseguy']]\n",
      "[['salad', 'viewer', 'aftershock'], ['meander', 'secret', 'gladiolus'], ['evening-wear', 'plover', 'dialogue']]\n",
      "[['slave', 'pail', 'today'], ['forte', 'prostanoid', 'cowboy'], ['system', 'schema', 'century']]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.backends.backend_pdf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.backends.backend_pdf\n",
    "import matplotlib\n",
    "import random\n",
    "\n",
    "training_strs, testing_strs = prepare_data()\n",
    "\n",
    "\n",
    "def train_with_schedule(total_eps=100, num_cycles=20, start_fraction_rem=0.2, end_fraction_rem=0.8,\n",
    "                        seed=0, lr=0.001, num=100, temperature=1):\n",
    "\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    eps_per_cycle = int(total_eps / num_cycles)\n",
    "\n",
    "    # Train on env. 0\n",
    "    train_on_env(training_strs, testing_strs, env=0, base_model='base_model_b8', generated_strs=[])\n",
    "    \n",
    "    perplexity_results = []\n",
    "    for cycle in range(num_cycles):\n",
    "        \n",
    "        current_fraction_rem = start_fraction_rem + (end_fraction_rem - start_fraction_rem) * cycle / (num_cycles - 1)\n",
    "        # Update the nrem_eps and rem_eps values for this cycle\n",
    "        rem_eps = int(current_fraction_rem * eps_per_cycle)\n",
    "        nrem_eps = eps_per_cycle - rem_eps\n",
    "        \n",
    "        # train for nrem_eps on real fmnist\n",
    "        print(\"NREM phase\")\n",
    "        train_on_env(training_strs, \n",
    "                     testing_strs, \n",
    "                     env=1, \n",
    "                     base_model='spatial_model_0' if cycle == 0 else f'spatial_model_1',\n",
    "                     generated_strs=[], \n",
    "                     num_train=num,\n",
    "                     eps=nrem_eps)\n",
    "        \n",
    "        # train for rem_eps on sampled mnist\n",
    "        print(\"REM phase\")\n",
    "        generated_strs = generative_replay(GPT(base_model='spatial_model_1'), num=num, temperature=temperature)\n",
    "        print(generated_strs[0:5])\n",
    "        train_on_env(training_strs, \n",
    "                     testing_strs, \n",
    "                     env=1, \n",
    "                     base_model=f'spatial_model_1',\n",
    "                     generated_strs=generated_strs, \n",
    "                     num_train=0,\n",
    "                     eps=rem_eps)\n",
    "    \n",
    "        for j in range(2):\n",
    "            with open(f\"spatial_model_{j}/test.txt\", 'r') as file:\n",
    "                test_data = [line.strip() for line in file]\n",
    "            perplexity = get_mean_perplexity(test_data, 'spatial_model_1')\n",
    "            perplexity_results.append([cycle, j, perplexity])\n",
    "        print(\"Perplexity results so far:\")\n",
    "        print(perplexity_results)\n",
    "\n",
    "    # Test on all environments\n",
    "    results = []\n",
    "    for i in range(2):\n",
    "        model = GPT(base_model=f'spatial_model_{i}')\n",
    "        for j in range(2):\n",
    "            with open(f\"spatial_model_{j}/test.txt\", 'r') as file:\n",
    "                test_data = [line.strip() for line in file]\n",
    "            accuracy = test_accuracy(model, test_data)\n",
    "            results.append([i, j, accuracy])\n",
    "\n",
    "    return results, perplexity_results\n",
    "\n",
    "\n",
    "def train_with_schedule_multiple_seeds(seeds=[0], total_eps=100, num_cycles=20, start_fraction_rem=0.2, end_fraction_rem=0.8,\n",
    "                                       lr=0.001, num=10):\n",
    "    \n",
    "    pdf_path = \"./outputs/total_eps={}_num_cycles={}_start_rem={}_end_rem={}_lr={}_num={}.pdf\".format(str(total_eps),\n",
    "                                                                                                                  str(num_cycles),\n",
    "                                                                                                                  str(start_fraction_rem),\n",
    "                                                                                                                  str(end_fraction_rem),\n",
    "                                                                                                                  str(lr),\n",
    "                                                                                                                  str(num))\n",
    "\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(pdf_path)\n",
    "    \n",
    "    eps_per_cycle = int(total_eps / num_cycles)\n",
    "    fig = plot_schedule(num_cycles, total_eps, eps_per_cycle, start_fraction_rem, end_fraction_rem)\n",
    "    pdf.savefig(fig, bbox_inches = \"tight\")\n",
    "    \n",
    "    results = [train_with_schedule(total_eps=total_eps,\n",
    "                                   num_cycles=num_cycles,\n",
    "                                   start_fraction_rem=start_fraction_rem,\n",
    "                                   end_fraction_rem=end_fraction_rem,\n",
    "                                   lr=lr,\n",
    "                                   seed=s,\n",
    "                                   num=num) for s in seeds]\n",
    "\n",
    "    fig = accuracy_paired_bar(results)\n",
    "    pdf.savefig(fig, bbox_inches = \"tight\")\n",
    "    fig = perplexity_plot(results)\n",
    "    pdf.savefig(fig, bbox_inches = \"tight\")\n",
    "    pdf.close()\n",
    "    return results\n",
    "   \n",
    "\n",
    "def plot_schedule(NUM_CYCLES, TOTAL_EPS, eps_per_cycle, starting_fraction_rem, ending_fraction_rem):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 2)\n",
    "\n",
    "    nrem_starts = list(range(0, TOTAL_EPS, int(eps_per_cycle)))\n",
    "    \n",
    "    for cycle in range(NUM_CYCLES):\n",
    "        # Calculate the current fraction of REM sleep for this cycle\n",
    "        current_fraction_rem = starting_fraction_rem + (ending_fraction_rem - starting_fraction_rem) * cycle / (NUM_CYCLES - 1)\n",
    "\n",
    "        # Update the nrem_eps and rem_eps values for this cycle\n",
    "        rem_eps = int(current_fraction_rem * eps_per_cycle)\n",
    "        nrem_eps = eps_per_cycle - rem_eps\n",
    "\n",
    "        rem_start = nrem_starts[cycle] + nrem_eps\n",
    "        \n",
    "        ax.broken_barh([(rem_start, rem_eps)],\n",
    "                       (10, 9),\n",
    "                       facecolors='tab:blue')\n",
    "        ax.broken_barh([(nrem_starts[cycle], nrem_eps)],\n",
    "                       (20, 9),\n",
    "                       facecolors='tab:red')\n",
    "\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_yticks([15, 25], labels=['REM', 'NREM'])\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d1894-b476-4e4f-9cc2-ed69157a474b",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26b198f3-4551-44ef-a911-27173f877dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to train model from base_model_b8 and save to spatial_model_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 20:47:53 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 20:47:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_0/runs/Feb12_20-47-53_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=20.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_0,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 20:47:54 - INFO - datasets.builder - Using custom data configuration default-d7eda5b506bf503a\n",
      "02/12/2024 20:47:54 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 20:47:54 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-d7eda5b506bf503a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-d7eda5b506bf503a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 15279.80it/s]\n",
      "02/12/2024 20:47:54 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 20:47:54 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2283.24it/s]\n",
      "02/12/2024 20:47:54 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 20:47:54 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 20:47:54 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-d7eda5b506bf503a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2064.13it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 20:47:54,104 >> loading configuration file base_model_b8/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 20:47:54,104 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"base_model_b8\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:47:54,113 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:47:54,113 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:47:54,113 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:47:54,113 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:47:54,113 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:47:54,113 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 20:47:54,150 >> loading weights file base_model_b8/model.safetensors\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:47:54,166 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 20:47:56,192 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 20:47:56,192 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at base_model_b8.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 20:47:56,193 >> loading configuration file base_model_b8/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:47:56,193 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|             | 0/1000 [00:00<?, ? examples/s]02/12/2024 20:47:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d7eda5b506bf503a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-c1662fe44234d4ad.arrow\n",
      "Running tokenizer on dataset:   0%|             | 0/1000 [00:00<?, ? examples/s]02/12/2024 20:47:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d7eda5b506bf503a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-227d9abab50e2a46.arrow\n",
      "Grouping texts in chunks of 1024:   0%|         | 0/1000 [00:00<?, ? examples/s]02/12/2024 20:47:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d7eda5b506bf503a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f9c7c744ec842ffd.arrow\n",
      "Grouping texts in chunks of 1024:   0%|         | 0/1000 [00:00<?, ? examples/s]02/12/2024 20:47:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d7eda5b506bf503a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-790b3e1555500c13.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 20:47:57,680 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 20:47:57,680 >>   Num examples = 20\n",
      "[INFO|trainer.py:1676] 2024-02-12 20:47:57,680 >>   Num Epochs = 20\n",
      "[INFO|trainer.py:1677] 2024-02-12 20:47:57,680 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 20:47:57,680 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 20:47:57,680 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 20:47:57,680 >>   Total optimization steps = 400\n",
      "[INFO|trainer.py:1683] 2024-02-12 20:47:57,680 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████| 400/400 [03:55<00:00,  1.71it/s][INFO|trainer.py:1912] 2024-02-12 20:51:52,966 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 235.2914, 'train_samples_per_second': 1.7, 'train_steps_per_second': 1.7, 'train_loss': 0.2355352210998535, 'epoch': 20.0}\n",
      "100%|█████████████████████████████████████████| 400/400 [03:55<00:00,  1.70it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 20:51:53,007 >> Saving model checkpoint to spatial_model_0\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 20:51:53,016 >> Configuration saved in spatial_model_0/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 20:51:53,019 >> Configuration saved in spatial_model_0/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 20:51:53,863 >> Model weights saved in spatial_model_0/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 20:51:53,872 >> tokenizer config file saved in spatial_model_0/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 20:51:53,872 >> Special tokens file saved in spatial_model_0/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       20.0\n",
      "  train_loss               =     0.2355\n",
      "  train_runtime            = 0:03:55.29\n",
      "  train_samples            =         20\n",
      "  train_samples_per_second =        1.7\n",
      "  train_steps_per_second   =        1.7\n",
      "02/12/2024 20:51:53 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 20:51:53,944 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 20:51:53,944 >>   Num examples = 20\n",
      "[INFO|trainer.py:3098] 2024-02-12 20:51:53,944 >>   Batch size = 1\n",
      "100%|███████████████████████████████████████████| 20/20 [00:02<00:00,  6.76it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       20.0\n",
      "  eval_accuracy           =     0.9313\n",
      "  eval_loss               =      0.184\n",
      "  eval_runtime            = 0:00:03.12\n",
      "  eval_samples            =         20\n",
      "  eval_samples_per_second =      6.394\n",
      "  eval_steps_per_second   =      6.394\n",
      "  perplexity              =      1.202\n",
      "[INFO|modelcard.py:452] 2024-02-12 20:51:57,310 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9312805474095797}]}\n",
      "NREM phase\n",
      "About to train model from spatial_model_0 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 20:52:05 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 20:52:05 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_20-52-04_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 20:52:05 - INFO - datasets.builder - Using custom data configuration default-248271d4fb8e4fa2\n",
      "02/12/2024 20:52:05 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 20:52:05 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-248271d4fb8e4fa2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-248271d4fb8e4fa2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 14716.86it/s]\n",
      "02/12/2024 20:52:05 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 20:52:05 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1612.88it/s]\n",
      "02/12/2024 20:52:05 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 20:52:05 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 20:52:05 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-248271d4fb8e4fa2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1848.12it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 20:52:05,797 >> loading configuration file spatial_model_0/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 20:52:05,798 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_0\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:52:05,810 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:52:05,810 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:52:05,810 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:52:05,810 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:52:05,810 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:52:05,810 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 20:52:05,868 >> loading weights file spatial_model_0/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:52:05,976 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 20:52:07,340 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 20:52:07,340 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 20:52:07,341 >> loading configuration file spatial_model_0/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:52:07,341 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:52:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-248271d4fb8e4fa2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-b63f234f934619d5.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:52:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-248271d4fb8e4fa2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-c615834ab0435f5d.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:52:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-248271d4fb8e4fa2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-d4583057ce7a434e.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:52:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-248271d4fb8e4fa2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-020f3674d8be239f.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 20:52:08,597 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 20:52:08,597 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 20:52:08,597 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 20:52:08,597 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 20:52:08,597 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 20:52:08,597 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 20:52:08,597 >>   Total optimization steps = 16\n",
      "[INFO|trainer.py:1683] 2024-02-12 20:52:08,597 >>   Number of trainable parameters = 124,439,808\n",
      "100%|███████████████████████████████████████████| 16/16 [00:10<00:00,  1.65it/s][INFO|trainer.py:1912] 2024-02-12 20:52:19,541 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 10.9455, 'train_samples_per_second': 1.462, 'train_steps_per_second': 1.462, 'train_loss': 0.3769145905971527, 'epoch': 8.0}\n",
      "100%|███████████████████████████████████████████| 16/16 [00:10<00:00,  1.46it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 20:52:19,549 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 20:52:19,550 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 20:52:19,550 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 20:52:20,167 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 20:52:20,167 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 20:52:20,168 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.3769\n",
      "  train_runtime            = 0:00:10.94\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.462\n",
      "  train_steps_per_second   =      1.462\n",
      "02/12/2024 20:52:20 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 20:52:20,202 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 20:52:20,202 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 20:52:20,202 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 11.79it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.9238\n",
      "  eval_loss               =     0.2682\n",
      "  eval_runtime            = 0:00:00.33\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.054\n",
      "  eval_steps_per_second   =      6.054\n",
      "  perplexity              =     1.3076\n",
      "[INFO|modelcard.py:452] 2024-02-12 20:52:20,800 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9237536656891495}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: pioneer, TO: championship, PATH: pioneer NORTH boom EAST championship', 'FROM: extent, TO: pioneer, PATH: extent WEST pin SOUTH pioneer', 'FROM: pin, TO: boom, PATH: pin EAST pioneer NORTH boom', 'FROM: boom, TO: apartment, PATH: boom SOUTH pioneer SOUTH pin EAST pioneer EAST apartment', 'FROM: signature, TO: pin, PATH: signature WEST boom SOUTH pioneer WEST pin']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 20:53:04 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 20:53:04 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_20-53-04_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 20:53:05 - INFO - datasets.builder - Using custom data configuration default-477134cd9ad976c9\n",
      "02/12/2024 20:53:05 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 20:53:05 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-477134cd9ad976c9/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-477134cd9ad976c9/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 16677.15it/s]\n",
      "02/12/2024 20:53:05 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 20:53:05 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2573.98it/s]\n",
      "02/12/2024 20:53:05 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 20:53:05 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 20:53:05 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-477134cd9ad976c9/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2017.95it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 20:53:05,933 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 20:53:05,934 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:53:05,942 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:53:05,942 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:53:05,942 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:53:05,942 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:53:05,942 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:53:05,942 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 20:53:05,978 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:53:06,049 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 20:53:07,366 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 20:53:07,366 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 20:53:07,367 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:53:07,367 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/102 [00:00<?, ? examples/s]02/12/2024 20:53:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-477134cd9ad976c9/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-e52eeb139264d023.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/102 [00:00<?, ? examples/s]02/12/2024 20:53:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-477134cd9ad976c9/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-ef03525100b5cdb2.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/102 [00:00<?, ? examples/s]02/12/2024 20:53:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-477134cd9ad976c9/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-be8a1da4ab94a97e.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/102 [00:00<?, ? examples/s]02/12/2024 20:53:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-477134cd9ad976c9/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-9d1d69cb3ade8aca.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 20:53:08,645 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 20:53:08,645 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 20:53:08,645 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 20:53:08,645 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 20:53:08,645 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 20:53:08,645 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 20:53:08,645 >>   Total optimization steps = 2\n",
      "[INFO|trainer.py:1683] 2024-02-12 20:53:08,645 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.03s/it][INFO|trainer.py:1912] 2024-02-12 20:53:10,768 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2.1233, 'train_samples_per_second': 0.942, 'train_steps_per_second': 0.942, 'train_loss': 0.8685132265090942, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.06s/it]\n",
      "[INFO|trainer.py:2816] 2024-02-12 20:53:10,771 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 20:53:10,772 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 20:53:10,772 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 20:53:11,346 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 20:53:11,347 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 20:53:11,347 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.8685\n",
      "  train_runtime            = 0:00:02.12\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      0.942\n",
      "  train_steps_per_second   =      0.942\n",
      "02/12/2024 20:53:11 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 20:53:11,380 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 20:53:11,380 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 20:53:11,380 >>   Batch size = 1\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 127.67it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.8553\n",
      "  eval_loss               =     0.7577\n",
      "  eval_runtime            = 0:00:00.15\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =      6.323\n",
      "  eval_steps_per_second   =      6.323\n",
      "  perplexity              =     2.1333\n",
      "[INFO|modelcard.py:452] 2024-02-12 20:53:11,674 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.855327468230694}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ac62f20dc7448ba43ef383994e762c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f54d6a752574b899d456b8b887b3df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.6787928283214568], [0, 1, 4.904977554082871]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 20:53:22 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 20:53:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_20-53-22_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 20:53:23 - INFO - datasets.builder - Using custom data configuration default-4687bc2762750a52\n",
      "02/12/2024 20:53:23 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 20:53:23 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-4687bc2762750a52/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-4687bc2762750a52/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 17586.18it/s]\n",
      "02/12/2024 20:53:23 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 20:53:23 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1836.79it/s]\n",
      "02/12/2024 20:53:23 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 20:53:23 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 20:53:23 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-4687bc2762750a52/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1848.12it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 20:53:23,649 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 20:53:23,649 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:53:23,658 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:53:23,658 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:53:23,658 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:53:23,658 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:53:23,658 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:53:23,658 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 20:53:23,692 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:53:23,762 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 20:53:25,088 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 20:53:25,088 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 20:53:25,089 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:53:25,089 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:53:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4687bc2762750a52/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-0f1445a469dd6894.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:53:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4687bc2762750a52/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-7a8de05263701b5b.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:53:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4687bc2762750a52/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-7b85efeffb8b1e45.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:53:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4687bc2762750a52/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-372a119f0b1f599c.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 20:53:26,331 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 20:53:26,331 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 20:53:26,331 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 20:53:26,331 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 20:53:26,331 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 20:53:26,331 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 20:53:26,331 >>   Total optimization steps = 16\n",
      "[INFO|trainer.py:1683] 2024-02-12 20:53:26,332 >>   Number of trainable parameters = 124,439,808\n",
      "100%|███████████████████████████████████████████| 16/16 [00:09<00:00,  1.77it/s][INFO|trainer.py:1912] 2024-02-12 20:53:36,309 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 9.9819, 'train_samples_per_second': 1.603, 'train_steps_per_second': 1.603, 'train_loss': 0.28072991967201233, 'epoch': 8.0}\n",
      "100%|███████████████████████████████████████████| 16/16 [00:09<00:00,  1.60it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 20:53:36,317 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 20:53:36,318 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 20:53:36,319 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 20:53:36,943 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 20:53:36,943 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 20:53:36,944 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.2807\n",
      "  train_runtime            = 0:00:09.98\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.603\n",
      "  train_steps_per_second   =      1.603\n",
      "02/12/2024 20:53:36 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 20:53:36,974 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 20:53:36,974 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 20:53:36,974 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 13.35it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.9267\n",
      "  eval_loss               =       0.22\n",
      "  eval_runtime            = 0:00:00.30\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.478\n",
      "  eval_steps_per_second   =      6.478\n",
      "  perplexity              =     1.2461\n",
      "[INFO|modelcard.py:452] 2024-02-12 20:53:37,418 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9266862170087976}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: championship, TO: pin, PATH: championship WEST boom SOUTH pioneer WEST pin', 'FROM: extent, TO: jet, PATH: extent EAST boom SOUTH pioneer SOUTH pilaf EAST jet', 'FROM: signature, TO: pilaf, PATH: signature SOUTH signature WEST pioneer SOUTH pilaf', 'FROM: signature, TO: jet, PATH: signature SOUTH signature SOUTH signature SOUTH signature EAST jet', 'FROM: jet, TO: pioneer, PATH: jet WEST pilaf NORTH pioneer']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 20:54:22 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 20:54:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_20-54-22_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 20:54:22 - INFO - datasets.builder - Using custom data configuration default-1ee12657fc48008f\n",
      "02/12/2024 20:54:22 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 20:54:22 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-1ee12657fc48008f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-1ee12657fc48008f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 18001.30it/s]\n",
      "02/12/2024 20:54:22 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 20:54:22 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2968.37it/s]\n",
      "02/12/2024 20:54:22 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 20:54:22 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 20:54:22 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-1ee12657fc48008f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2016.01it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 20:54:22,878 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 20:54:22,878 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:54:22,887 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:54:22,887 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:54:22,887 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:54:22,887 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:54:22,887 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:54:22,887 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 20:54:22,928 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:54:23,005 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 20:54:24,343 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 20:54:24,343 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 20:54:24,343 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:54:24,344 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/103 [00:00<?, ? examples/s]02/12/2024 20:54:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1ee12657fc48008f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-04fa56142db18cc9.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/103 [00:00<?, ? examples/s]02/12/2024 20:54:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1ee12657fc48008f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-569886b250aa028d.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/103 [00:00<?, ? examples/s]02/12/2024 20:54:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1ee12657fc48008f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-910906dce8a9bc27.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/103 [00:00<?, ? examples/s]02/12/2024 20:54:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1ee12657fc48008f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-7f721627a4a56f73.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 20:54:25,531 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 20:54:25,531 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 20:54:25,531 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 20:54:25,531 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 20:54:25,531 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 20:54:25,531 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 20:54:25,531 >>   Total optimization steps = 2\n",
      "[INFO|trainer.py:1683] 2024-02-12 20:54:25,531 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.06s/it][INFO|trainer.py:1912] 2024-02-12 20:54:27,721 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2.1911, 'train_samples_per_second': 0.913, 'train_steps_per_second': 0.913, 'train_loss': 0.8454174995422363, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.10s/it]\n",
      "[INFO|trainer.py:2816] 2024-02-12 20:54:27,729 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 20:54:27,730 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 20:54:27,731 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 20:54:28,303 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 20:54:28,308 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 20:54:28,308 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.8454\n",
      "  train_runtime            = 0:00:02.19\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      0.913\n",
      "  train_steps_per_second   =      0.913\n",
      "02/12/2024 20:54:28 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 20:54:28,350 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 20:54:28,350 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 20:54:28,350 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 33.17it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.8504\n",
      "  eval_loss               =     0.7434\n",
      "  eval_runtime            = 0:00:00.20\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =      4.928\n",
      "  eval_steps_per_second   =      4.928\n",
      "  perplexity              =     2.1031\n",
      "[INFO|modelcard.py:452] 2024-02-12 20:54:28,720 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8504398826979472}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2521ed1dcbf445caa5c96f65119f386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddeea2940e0b4e58a0b2f9d9ddbe2276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.6787928283214568], [0, 1, 4.904977554082871], [1, 0, 1.6949962705373764], [1, 1, 3.3475213170051576]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 20:54:39 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 20:54:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_20-54-39_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 20:54:40 - INFO - datasets.builder - Using custom data configuration default-1baba681688efb37\n",
      "02/12/2024 20:54:40 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 20:54:40 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-1baba681688efb37/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-1baba681688efb37/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 18682.87it/s]\n",
      "02/12/2024 20:54:40 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 20:54:40 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2669.83it/s]\n",
      "02/12/2024 20:54:40 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 20:54:40 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 20:54:40 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-1baba681688efb37/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2083.09it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 20:54:40,479 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 20:54:40,480 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:54:40,488 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:54:40,488 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:54:40,488 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:54:40,488 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:54:40,488 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:54:40,488 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 20:54:40,523 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:54:40,598 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 20:54:41,909 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 20:54:41,909 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 20:54:41,910 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:54:41,910 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:54:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1baba681688efb37/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-b8c9851b50d10190.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:54:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1baba681688efb37/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-78872819b04ab63b.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:54:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1baba681688efb37/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-8d49d1de65bdd3a3.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:54:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1baba681688efb37/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-545789eabf6de000.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 20:54:43,369 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 20:54:43,369 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 20:54:43,369 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 20:54:43,369 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 20:54:43,369 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 20:54:43,369 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 20:54:43,369 >>   Total optimization steps = 16\n",
      "[INFO|trainer.py:1683] 2024-02-12 20:54:43,369 >>   Number of trainable parameters = 124,439,808\n",
      "100%|███████████████████████████████████████████| 16/16 [00:10<00:00,  1.72it/s][INFO|trainer.py:1912] 2024-02-12 20:54:53,772 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 10.4095, 'train_samples_per_second': 1.537, 'train_steps_per_second': 1.537, 'train_loss': 0.2394648790359497, 'epoch': 8.0}\n",
      "100%|███████████████████████████████████████████| 16/16 [00:10<00:00,  1.54it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 20:54:53,783 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 20:54:53,784 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 20:54:53,784 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 20:54:54,398 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 20:54:54,399 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 20:54:54,399 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.2395\n",
      "  train_runtime            = 0:00:10.40\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.537\n",
      "  train_steps_per_second   =      1.537\n",
      "02/12/2024 20:54:54 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 20:54:54,434 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 20:54:54,434 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 20:54:54,434 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 12.50it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.9326\n",
      "  eval_loss               =     0.1939\n",
      "  eval_runtime            = 0:00:00.33\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      5.963\n",
      "  eval_steps_per_second   =      5.963\n",
      "  perplexity              =      1.214\n",
      "[INFO|modelcard.py:452] 2024-02-12 20:54:55,203 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9325513196480938}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: pioneer, TO: signature, PATH: pioneer EAST signature', 'FROM: championship, TO: boom, PATH: championship NORTH signature NORTH pioneer WEST boom', 'FROM: signature, TO: championship, PATH: signature SOUTH championship', 'FROM: pioneer, TO: pioneer, PATH: pioneer', 'FROM: pioneer, TO: boom, PATH: pioneer WEST boom']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 20:55:38 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 20:55:38 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_20-55-38_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 20:55:40 - INFO - datasets.builder - Using custom data configuration default-dbe164fb11df2824\n",
      "02/12/2024 20:55:40 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 20:55:40 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-dbe164fb11df2824/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-dbe164fb11df2824/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 17403.75it/s]\n",
      "02/12/2024 20:55:40 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 20:55:40 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2207.53it/s]\n",
      "02/12/2024 20:55:40 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 20:55:40 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 20:55:40 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-dbe164fb11df2824/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2202.89it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 20:55:40,347 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 20:55:40,347 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:55:40,356 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:55:40,356 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:55:40,356 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:55:40,356 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:55:40,356 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:55:40,356 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 20:55:40,396 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:55:40,467 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 20:55:41,790 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 20:55:41,790 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 20:55:41,790 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:55:41,790 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/108 [00:00<?, ? examples/s]02/12/2024 20:55:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-dbe164fb11df2824/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-1a0e022b6ff7a236.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/108 [00:00<?, ? examples/s]02/12/2024 20:55:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-dbe164fb11df2824/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a2d752326a23f845.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/108 [00:00<?, ? examples/s]02/12/2024 20:55:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-dbe164fb11df2824/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-949a46a1366e1326.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/108 [00:00<?, ? examples/s]02/12/2024 20:55:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-dbe164fb11df2824/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-258c8b5b684e07d0.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 20:55:43,147 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 20:55:43,147 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 20:55:43,147 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 20:55:43,147 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 20:55:43,147 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 20:55:43,147 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 20:55:43,147 >>   Total optimization steps = 2\n",
      "[INFO|trainer.py:1683] 2024-02-12 20:55:43,147 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.03s/it][INFO|trainer.py:1912] 2024-02-12 20:55:45,280 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2.1343, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.937, 'train_loss': 0.9960825443267822, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.07s/it]\n",
      "[INFO|trainer.py:2816] 2024-02-12 20:55:45,289 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 20:55:45,290 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 20:55:45,291 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 20:55:45,862 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 20:55:45,868 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 20:55:45,868 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.9961\n",
      "  train_runtime            = 0:00:02.13\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      0.937\n",
      "  train_steps_per_second   =      0.937\n",
      "02/12/2024 20:55:45 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 20:55:45,914 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 20:55:45,914 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 20:55:45,914 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 35.30it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.8504\n",
      "  eval_loss               =     0.8576\n",
      "  eval_runtime            = 0:00:00.20\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =       4.87\n",
      "  eval_steps_per_second   =       4.87\n",
      "  perplexity              =     2.3575\n",
      "[INFO|modelcard.py:452] 2024-02-12 20:55:46,430 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8504398826979472}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59aad118bf904b7997f49b9a5ca2ce3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a5becef1d4495ba6dcf5716fa12a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.6787928283214568], [0, 1, 4.904977554082871], [1, 0, 1.6949962705373764], [1, 1, 3.3475213170051576], [2, 0, 1.7425754994153977], [2, 1, 2.7078740179538725]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 20:55:59 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 20:55:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_20-55-58_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 20:55:59 - INFO - datasets.builder - Using custom data configuration default-4e9b192f0f26df99\n",
      "02/12/2024 20:55:59 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 20:55:59 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-4e9b192f0f26df99/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-4e9b192f0f26df99/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 16384.00it/s]\n",
      "02/12/2024 20:55:59 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 20:55:59 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2438.55it/s]\n",
      "02/12/2024 20:55:59 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 20:55:59 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 20:55:59 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-4e9b192f0f26df99/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2102.94it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 20:55:59,585 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 20:55:59,586 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:55:59,594 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:55:59,594 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:55:59,594 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:55:59,594 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:55:59,594 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:55:59,594 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 20:55:59,628 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:55:59,695 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 20:56:01,020 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 20:56:01,020 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 20:56:01,020 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:56:01,021 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:56:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4e9b192f0f26df99/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-302acd72c34463e0.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:56:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4e9b192f0f26df99/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-9924f569e339c58d.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:56:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4e9b192f0f26df99/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-109c578b887e8902.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:56:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4e9b192f0f26df99/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-30df782217d8c431.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 20:56:02,239 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 20:56:02,239 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 20:56:02,239 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 20:56:02,239 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 20:56:02,239 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 20:56:02,239 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 20:56:02,239 >>   Total optimization steps = 16\n",
      "[INFO|trainer.py:1683] 2024-02-12 20:56:02,239 >>   Number of trainable parameters = 124,439,808\n",
      "100%|███████████████████████████████████████████| 16/16 [00:11<00:00,  1.64it/s][INFO|trainer.py:1912] 2024-02-12 20:56:13,841 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 11.6046, 'train_samples_per_second': 1.379, 'train_steps_per_second': 1.379, 'train_loss': 0.2535225450992584, 'epoch': 8.0}\n",
      "100%|███████████████████████████████████████████| 16/16 [00:11<00:00,  1.38it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 20:56:13,848 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 20:56:13,849 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 20:56:13,849 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 20:56:14,537 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 20:56:14,546 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 20:56:14,546 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.2535\n",
      "  train_runtime            = 0:00:11.60\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.379\n",
      "  train_steps_per_second   =      1.379\n",
      "02/12/2024 20:56:14 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 20:56:14,598 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 20:56:14,598 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 20:56:14,598 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 10.36it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.9277\n",
      "  eval_loss               =     0.2131\n",
      "  eval_runtime            = 0:00:00.35\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      5.703\n",
      "  eval_steps_per_second   =      5.703\n",
      "  perplexity              =     1.2375\n",
      "[INFO|modelcard.py:452] 2024-02-12 20:56:15,176 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.927663734115347}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: apartment, TO: pioneer, PATH: apartment NORTH signature WEST pioneer', 'FROM: extent, TO: signature, PATH: extent EAST boom SOUTH pioneer EAST signature', 'FROM: signature, TO: boom, PATH: signature WEST pioneer NORTH boom', 'FROM: signature, TO: boom, PATH: signature NORTH boom', 'FROM: pioneer, TO: boom, PATH: pioneer NORTH boom']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 20:56:59 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 20:56:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_20-56-58_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 20:57:00 - INFO - datasets.builder - Using custom data configuration default-d5f673e9965c11c2\n",
      "02/12/2024 20:57:00 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 20:57:00 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-d5f673e9965c11c2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-d5f673e9965c11c2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 18893.26it/s]\n",
      "02/12/2024 20:57:00 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 20:57:00 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1107.55it/s]\n",
      "02/12/2024 20:57:00 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 20:57:00 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 20:57:00 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-d5f673e9965c11c2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2107.69it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 20:57:00,170 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 20:57:00,171 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:57:00,180 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:57:00,180 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:57:00,180 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:57:00,180 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:57:00,180 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:57:00,180 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 20:57:00,223 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:57:00,300 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 20:57:01,626 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 20:57:01,626 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 20:57:01,626 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:57:01,627 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/103 [00:00<?, ? examples/s]02/12/2024 20:57:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d5f673e9965c11c2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-26b186976b52d12b.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/103 [00:00<?, ? examples/s]02/12/2024 20:57:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d5f673e9965c11c2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-fe8c45407aeaa0fe.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/103 [00:00<?, ? examples/s]02/12/2024 20:57:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d5f673e9965c11c2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-70951b407c9b9cbf.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/103 [00:00<?, ? examples/s]02/12/2024 20:57:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d5f673e9965c11c2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f0c56c502eec88a1.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 20:57:02,812 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 20:57:02,812 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 20:57:02,812 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 20:57:02,812 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 20:57:02,812 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 20:57:02,812 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 20:57:02,812 >>   Total optimization steps = 2\n",
      "[INFO|trainer.py:1683] 2024-02-12 20:57:02,812 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.06s/it][INFO|trainer.py:1912] 2024-02-12 20:57:05,041 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2.2299, 'train_samples_per_second': 0.897, 'train_steps_per_second': 0.897, 'train_loss': 0.8481770157814026, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.11s/it]\n",
      "[INFO|trainer.py:2816] 2024-02-12 20:57:05,045 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 20:57:05,047 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 20:57:05,047 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 20:57:05,642 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 20:57:05,643 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 20:57:05,643 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.8482\n",
      "  train_runtime            = 0:00:02.22\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      0.897\n",
      "  train_steps_per_second   =      0.897\n",
      "02/12/2024 20:57:05 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 20:57:05,678 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 20:57:05,678 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 20:57:05,678 >>   Batch size = 1\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 102.89it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.8573\n",
      "  eval_loss               =     0.7295\n",
      "  eval_runtime            = 0:00:00.16\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =      6.066\n",
      "  eval_steps_per_second   =      6.066\n",
      "  perplexity              =     2.0741\n",
      "[INFO|modelcard.py:452] 2024-02-12 20:57:06,029 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8572825024437928}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1f07df373b4de7b3632ee76068d8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fecd42f1ce2f40ba867340a749598112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.6787928283214568], [0, 1, 4.904977554082871], [1, 0, 1.6949962705373764], [1, 1, 3.3475213170051576], [2, 0, 1.7425754994153977], [2, 1, 2.7078740179538725], [3, 0, 1.7362119734287262], [3, 1, 2.3355567395687102]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 20:57:17 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 20:57:17 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_20-57-17_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 20:57:18 - INFO - datasets.builder - Using custom data configuration default-4792b18067f0d6a4\n",
      "02/12/2024 20:57:18 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 20:57:18 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-4792b18067f0d6a4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-4792b18067f0d6a4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 17084.74it/s]\n",
      "02/12/2024 20:57:18 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 20:57:18 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2687.79it/s]\n",
      "02/12/2024 20:57:18 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 20:57:18 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 20:57:18 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-4792b18067f0d6a4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2125.31it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 20:57:18,246 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 20:57:18,247 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:57:18,255 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:57:18,255 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:57:18,255 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:57:18,255 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:57:18,255 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:57:18,255 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 20:57:18,292 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:57:18,364 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 20:57:19,702 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 20:57:19,702 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 20:57:19,703 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:57:19,703 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:57:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4792b18067f0d6a4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-935775abe19549c7.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:57:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4792b18067f0d6a4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a54eac905f9ef378.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:57:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4792b18067f0d6a4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-7c96fdfc3c9c68b4.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:57:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4792b18067f0d6a4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-53f4ffd9fc4dff94.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 20:57:20,925 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 20:57:20,925 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 20:57:20,925 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 20:57:20,925 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 20:57:20,925 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 20:57:20,925 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 20:57:20,925 >>   Total optimization steps = 16\n",
      "[INFO|trainer.py:1683] 2024-02-12 20:57:20,925 >>   Number of trainable parameters = 124,439,808\n",
      "100%|███████████████████████████████████████████| 16/16 [00:10<00:00,  1.68it/s][INFO|trainer.py:1912] 2024-02-12 20:57:31,126 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 10.2028, 'train_samples_per_second': 1.568, 'train_steps_per_second': 1.568, 'train_loss': 0.21752086281776428, 'epoch': 8.0}\n",
      "100%|███████████████████████████████████████████| 16/16 [00:10<00:00,  1.57it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 20:57:31,131 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 20:57:31,133 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 20:57:31,133 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 20:57:31,783 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 20:57:31,784 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 20:57:31,785 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.2175\n",
      "  train_runtime            = 0:00:10.20\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.568\n",
      "  train_steps_per_second   =      1.568\n",
      "02/12/2024 20:57:31 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 20:57:31,817 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 20:57:31,817 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 20:57:31,817 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 12.75it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.9374\n",
      "  eval_loss               =     0.1804\n",
      "  eval_runtime            = 0:00:00.31\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.261\n",
      "  eval_steps_per_second   =      6.261\n",
      "  perplexity              =     1.1977\n",
      "[INFO|modelcard.py:452] 2024-02-12 20:57:32,290 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9374389051808406}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: pin, TO: championship, PATH: pin EAST pioneer EAST signature SOUTH championship', 'FROM: pioneer, TO: extent, PATH: pioneer WEST pin NORTH extent', 'FROM: extent, TO: pioneer, PATH: extent EAST boom SOUTH pioneer', 'FROM: signature, TO: pin, PATH: signature SOUTH championship WEST boom WEST extent SOUTH pin', 'FROM: championship, TO: pioneer, PATH: championship WEST pioneer']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 20:58:16 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 20:58:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_20-58-15_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 20:58:16 - INFO - datasets.builder - Using custom data configuration default-1518344e15f3c8fc\n",
      "02/12/2024 20:58:16 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 20:58:16 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-1518344e15f3c8fc/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-1518344e15f3c8fc/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 16256.99it/s]\n",
      "02/12/2024 20:58:16 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 20:58:16 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2522.13it/s]\n",
      "02/12/2024 20:58:16 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 20:58:16 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 20:58:16 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-1518344e15f3c8fc/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1458.89it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 20:58:16,731 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 20:58:16,732 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:58:16,741 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:58:16,741 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:58:16,741 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:58:16,741 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:58:16,741 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:58:16,741 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 20:58:16,778 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:58:16,849 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 20:58:18,179 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 20:58:18,179 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 20:58:18,180 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:58:18,180 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:58:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1518344e15f3c8fc/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-072852738f9ebdef.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:58:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1518344e15f3c8fc/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-3a210742625ef978.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:58:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1518344e15f3c8fc/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-5e6fc332967e5e1d.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:58:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1518344e15f3c8fc/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-32ac751955a32e55.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 20:58:19,540 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 20:58:19,540 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 20:58:19,540 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 20:58:19,540 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 20:58:19,540 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 20:58:19,540 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 20:58:19,540 >>   Total optimization steps = 2\n",
      "[INFO|trainer.py:1683] 2024-02-12 20:58:19,541 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.02it/s][INFO|trainer.py:1912] 2024-02-12 20:58:21,555 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2.0155, 'train_samples_per_second': 0.992, 'train_steps_per_second': 0.992, 'train_loss': 0.9735227823257446, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.01s/it]\n",
      "[INFO|trainer.py:2816] 2024-02-12 20:58:21,565 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 20:58:21,566 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 20:58:21,567 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 20:58:22,184 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 20:58:22,189 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 20:58:22,189 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.9735\n",
      "  train_runtime            = 0:00:02.01\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      0.992\n",
      "  train_steps_per_second   =      0.992\n",
      "02/12/2024 20:58:22 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 20:58:22,237 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 20:58:22,237 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 20:58:22,237 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.02it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.8368\n",
      "  eval_loss               =     0.8299\n",
      "  eval_runtime            = 0:00:00.20\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =      4.796\n",
      "  eval_steps_per_second   =      4.796\n",
      "  perplexity              =     2.2931\n",
      "[INFO|modelcard.py:452] 2024-02-12 20:58:22,623 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8367546432062561}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5dbc1b03e24ef98bd20e20e6767217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873971b115a94f988ccb68e1d4470ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.6787928283214568], [0, 1, 4.904977554082871], [1, 0, 1.6949962705373764], [1, 1, 3.3475213170051576], [2, 0, 1.7425754994153977], [2, 1, 2.7078740179538725], [3, 0, 1.7362119734287262], [3, 1, 2.3355567395687102], [4, 0, 1.8724202185869216], [4, 1, 2.0651079058647155]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 20:58:33 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 20:58:33 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_20-58-33_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 20:58:34 - INFO - datasets.builder - Using custom data configuration default-f8425c87219ad226\n",
      "02/12/2024 20:58:34 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 20:58:34 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-f8425c87219ad226/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-f8425c87219ad226/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 16677.15it/s]\n",
      "02/12/2024 20:58:34 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 20:58:34 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 3018.57it/s]\n",
      "02/12/2024 20:58:34 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 20:58:34 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 20:58:34 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-f8425c87219ad226/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2118.87it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 20:58:34,408 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 20:58:34,409 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:58:34,417 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:58:34,417 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:58:34,417 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:58:34,417 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:58:34,417 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:58:34,417 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 20:58:34,456 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:58:34,528 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 20:58:35,866 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 20:58:35,866 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 20:58:35,867 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:58:35,867 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:58:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-f8425c87219ad226/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-9a11592bb1887f70.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 20:58:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-f8425c87219ad226/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-4968257fa20cf864.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:58:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-f8425c87219ad226/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-3af381cf66826b86.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 20:58:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-f8425c87219ad226/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-8dc7697d700db182.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 20:58:37,396 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 20:58:37,396 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 20:58:37,396 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 20:58:37,396 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 20:58:37,396 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 20:58:37,396 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 20:58:37,396 >>   Total optimization steps = 16\n",
      "[INFO|trainer.py:1683] 2024-02-12 20:58:37,396 >>   Number of trainable parameters = 124,439,808\n",
      "100%|███████████████████████████████████████████| 16/16 [00:10<00:00,  1.74it/s][INFO|trainer.py:1912] 2024-02-12 20:58:47,575 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 10.1805, 'train_samples_per_second': 1.572, 'train_steps_per_second': 1.572, 'train_loss': 0.21548034250736237, 'epoch': 8.0}\n",
      "100%|███████████████████████████████████████████| 16/16 [00:10<00:00,  1.57it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 20:58:47,581 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 20:58:47,582 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 20:58:47,583 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 20:58:48,188 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 20:58:48,189 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 20:58:48,189 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.2155\n",
      "  train_runtime            = 0:00:10.18\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.572\n",
      "  train_steps_per_second   =      1.572\n",
      "02/12/2024 20:58:48 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 20:58:48,219 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 20:58:48,219 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 20:58:48,219 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 13.25it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.9311\n",
      "  eval_loss               =     0.1824\n",
      "  eval_runtime            = 0:00:00.30\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =        6.6\n",
      "  eval_steps_per_second   =        6.6\n",
      "  perplexity              =     1.2001\n",
      "[INFO|modelcard.py:452] 2024-02-12 20:58:49,348 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9310850439882697}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: dictaphone, TO: technician, PATH: dictaphone SOUTH nucleotidase EAST backburn EAST technician', 'FROM: technician, TO: technician, PATH: technician SOUTH forum WEST technician', 'FROM: dictaphone, TO: technician, PATH: dictaphone EAST grape SOUTH backburn EAST technician', 'FROM: forum, TO: grape, PATH: forum WEST technician NORTH grape', 'FROM: dictaphone, TO: metabolite, PATH: dictaphone SOUTH nucleotidase SOUTH metabolite']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 20:59:43 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 20:59:43 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_20-59-42_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 20:59:43 - INFO - datasets.builder - Using custom data configuration default-4265a88f771103a3\n",
      "02/12/2024 20:59:43 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 20:59:43 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-4265a88f771103a3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-4265a88f771103a3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 18724.57it/s]\n",
      "02/12/2024 20:59:43 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 20:59:43 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2257.43it/s]\n",
      "02/12/2024 20:59:43 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 20:59:43 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 20:59:43 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-4265a88f771103a3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2270.26it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 20:59:43,929 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 20:59:43,929 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:59:43,937 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:59:43,937 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:59:43,937 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:59:43,937 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:59:43,937 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 20:59:43,937 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 20:59:43,974 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:59:44,050 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 20:59:45,513 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 20:59:45,513 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 20:59:45,514 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 20:59:45,514 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/101 [00:00<?, ? examples/s]02/12/2024 20:59:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4265a88f771103a3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-1b561df7c3d6a55d.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/101 [00:00<?, ? examples/s]02/12/2024 20:59:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4265a88f771103a3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-e3525fc016bb9a80.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/101 [00:00<?, ? examples/s]02/12/2024 20:59:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4265a88f771103a3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a3528a8cdd684eaf.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/101 [00:00<?, ? examples/s]02/12/2024 20:59:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4265a88f771103a3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f269bf2fc313c609.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 20:59:46,784 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 20:59:46,784 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 20:59:46,784 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 20:59:46,784 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 20:59:46,784 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 20:59:46,784 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 20:59:46,784 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 20:59:46,784 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.18it/s][INFO|trainer.py:1912] 2024-02-12 20:59:50,645 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3.8616, 'train_samples_per_second': 1.036, 'train_steps_per_second': 1.036, 'train_loss': 0.7646007537841797, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.04it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 20:59:50,651 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 20:59:50,652 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 20:59:50,653 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 20:59:51,306 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 20:59:51,308 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 20:59:51,308 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.7646\n",
      "  train_runtime            = 0:00:03.86\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.036\n",
      "  train_steps_per_second   =      1.036\n",
      "02/12/2024 20:59:51 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 20:59:51,349 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 20:59:51,349 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 20:59:51,349 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 10.16it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.8578\n",
      "  eval_loss               =     0.6863\n",
      "  eval_runtime            = 0:00:00.37\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      5.358\n",
      "  eval_steps_per_second   =      5.358\n",
      "  perplexity              =     1.9863\n",
      "[INFO|modelcard.py:452] 2024-02-12 20:59:52,273 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8577712609970675}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fead24101063406796fdb3984beeb79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeec600d91214a66882db167ee331be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.6787928283214568], [0, 1, 4.904977554082871], [1, 0, 1.6949962705373764], [1, 1, 3.3475213170051576], [2, 0, 1.7425754994153977], [2, 1, 2.7078740179538725], [3, 0, 1.7362119734287262], [3, 1, 2.3355567395687102], [4, 0, 1.8724202185869216], [4, 1, 2.0651079058647155], [5, 0, 2.3775364249944686], [5, 1, 2.2084951758384705]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:00:05 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:00:05 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-00-05_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:00:06 - INFO - datasets.builder - Using custom data configuration default-b6c71fe88dd33155\n",
      "02/12/2024 21:00:06 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:00:06 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-b6c71fe88dd33155/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-b6c71fe88dd33155/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 22919.69it/s]\n",
      "02/12/2024 21:00:06 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:00:06 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 3252.66it/s]\n",
      "02/12/2024 21:00:06 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:00:06 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:00:06 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-b6c71fe88dd33155/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2506.31it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:00:06,386 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:00:06,386 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:00:06,394 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:00:06,394 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:00:06,394 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:00:06,394 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:00:06,394 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:00:06,394 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:00:06,430 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:00:06,504 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:00:07,969 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:00:07,969 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:00:07,970 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:00:07,970 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:00:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-b6c71fe88dd33155/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-85096a6e57f167c4.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:00:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-b6c71fe88dd33155/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-3026bdf257c27d21.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:00:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-b6c71fe88dd33155/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-59f800d746673fd7.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:00:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-b6c71fe88dd33155/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-54cd7635fbabec0b.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:00:09,122 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:00:09,122 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:00:09,122 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:00:09,122 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:00:09,122 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:00:09,122 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:00:09,122 >>   Total optimization steps = 16\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:00:09,123 >>   Number of trainable parameters = 124,439,808\n",
      "100%|███████████████████████████████████████████| 16/16 [00:11<00:00,  1.61it/s][INFO|trainer.py:1912] 2024-02-12 21:00:20,493 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 11.3776, 'train_samples_per_second': 1.406, 'train_steps_per_second': 1.406, 'train_loss': 0.2039608359336853, 'epoch': 8.0}\n",
      "100%|███████████████████████████████████████████| 16/16 [00:11<00:00,  1.41it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:00:20,504 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:00:20,506 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:00:20,506 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:00:21,212 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:00:21,214 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:00:21,214 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =      0.204\n",
      "  train_runtime            = 0:00:11.37\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.406\n",
      "  train_steps_per_second   =      1.406\n",
      "02/12/2024 21:00:21 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:00:21,248 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:00:21,248 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:00:21,248 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 11.67it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =      0.937\n",
      "  eval_loss               =     0.1729\n",
      "  eval_runtime            = 0:00:00.35\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      5.559\n",
      "  eval_steps_per_second   =      5.559\n",
      "  perplexity              =     1.1888\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:00:21,774 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9369501466275659}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: nucleotidase, TO: backburn, PATH: nucleotidase EAST backburn', 'FROM: ideology, TO: grape, PATH: ideology WEST grape', 'FROM: grape, TO: metabolite, PATH: grape SOUTH backburn EAST metabolite', 'FROM: backburn, TO: nucleotidase, PATH: backburn WEST nucleotidase', 'FROM: maestro, TO: metabolite, PATH: maestro EAST ovary EAST metabolite']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:01:17 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:01:17 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-01-16_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:01:17 - INFO - datasets.builder - Using custom data configuration default-d56838700ef238fe\n",
      "02/12/2024 21:01:17 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:01:17 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-d56838700ef238fe/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-d56838700ef238fe/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 20867.18it/s]\n",
      "02/12/2024 21:01:17 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:01:17 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1038.32it/s]\n",
      "02/12/2024 21:01:17 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:01:17 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:01:17 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-d56838700ef238fe/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2325.65it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:01:17,689 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:01:17,690 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:01:17,698 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:01:17,698 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:01:17,698 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:01:17,698 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:01:17,698 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:01:17,698 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:01:17,739 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:01:17,815 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:01:19,255 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:01:19,255 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:01:19,256 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:01:19,256 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/101 [00:00<?, ? examples/s]02/12/2024 21:01:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d56838700ef238fe/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a159339b7efeda51.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/101 [00:00<?, ? examples/s]02/12/2024 21:01:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d56838700ef238fe/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-be35654d01b7d315.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/101 [00:00<?, ? examples/s]02/12/2024 21:01:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d56838700ef238fe/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-9867e436daeb323e.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/101 [00:00<?, ? examples/s]02/12/2024 21:01:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d56838700ef238fe/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-fcd8e07e8462603c.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:01:20,599 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:01:20,599 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:01:20,599 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:01:20,599 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:01:20,599 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:01:20,599 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:01:20,599 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:01:20,599 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.13it/s][INFO|trainer.py:1912] 2024-02-12 21:01:24,703 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 4.1054, 'train_samples_per_second': 0.974, 'train_steps_per_second': 0.974, 'train_loss': 0.630646824836731, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.03s/it]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:01:24,708 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:01:24,710 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:01:24,711 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:01:25,385 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:01:25,386 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:01:25,386 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.6306\n",
      "  train_runtime            = 0:00:04.10\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      0.974\n",
      "  train_steps_per_second   =      0.974\n",
      "02/12/2024 21:01:25 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:01:25,417 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:01:25,417 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:01:25,417 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 12.63it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.8705\n",
      "  eval_loss               =     0.5453\n",
      "  eval_runtime            = 0:00:00.31\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.328\n",
      "  eval_steps_per_second   =      6.328\n",
      "  perplexity              =     1.7251\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:01:26,055 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8704789833822092}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6bad68e0c04213bc141485c5cf5e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12ea3b84c234d97bbc6ed381416cc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.6787928283214568], [0, 1, 4.904977554082871], [1, 0, 1.6949962705373764], [1, 1, 3.3475213170051576], [2, 0, 1.7425754994153977], [2, 1, 2.7078740179538725], [3, 0, 1.7362119734287262], [3, 1, 2.3355567395687102], [4, 0, 1.8724202185869216], [4, 1, 2.0651079058647155], [5, 0, 2.3775364249944686], [5, 1, 2.2084951758384705], [6, 0, 2.7467838525772095], [6, 1, 1.8725458562374115]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:01:37 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:01:37 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-01-37_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:01:38 - INFO - datasets.builder - Using custom data configuration default-8c4ca09f52b1e1c0\n",
      "02/12/2024 21:01:38 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:01:38 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-8c4ca09f52b1e1c0/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-8c4ca09f52b1e1c0/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 15505.74it/s]\n",
      "02/12/2024 21:01:38 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:01:38 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2522.89it/s]\n",
      "02/12/2024 21:01:38 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:01:38 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:01:38 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-8c4ca09f52b1e1c0/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1833.17it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:01:38,185 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:01:38,186 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:01:38,195 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:01:38,195 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:01:38,195 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:01:38,195 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:01:38,195 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:01:38,195 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:01:38,233 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:01:38,304 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:01:39,631 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:01:39,631 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:01:39,632 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:01:39,632 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:01:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-8c4ca09f52b1e1c0/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-467194ffd23910ec.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:01:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-8c4ca09f52b1e1c0/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-447d0d0a9cba3a64.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:01:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-8c4ca09f52b1e1c0/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-9972e5baabaf8e79.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:01:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-8c4ca09f52b1e1c0/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-e35676f41212e095.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:01:41,158 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:01:41,158 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:01:41,158 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:01:41,158 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:01:41,158 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:01:41,158 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:01:41,158 >>   Total optimization steps = 16\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:01:41,158 >>   Number of trainable parameters = 124,439,808\n",
      "100%|███████████████████████████████████████████| 16/16 [00:11<00:00,  1.40it/s][INFO|trainer.py:1912] 2024-02-12 21:01:53,064 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 11.9081, 'train_samples_per_second': 1.344, 'train_steps_per_second': 1.344, 'train_loss': 0.20008611679077148, 'epoch': 8.0}\n",
      "100%|███████████████████████████████████████████| 16/16 [00:11<00:00,  1.34it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:01:53,077 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:01:53,081 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:01:53,083 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:01:53,967 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:01:53,972 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:01:53,972 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.2001\n",
      "  train_runtime            = 0:00:11.90\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.344\n",
      "  train_steps_per_second   =      1.344\n",
      "02/12/2024 21:01:54 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:01:54,035 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:01:54,035 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:01:54,036 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  9.24it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =      0.935\n",
      "  eval_loss               =     0.1709\n",
      "  eval_runtime            = 0:00:00.48\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      4.133\n",
      "  eval_steps_per_second   =      4.133\n",
      "  perplexity              =     1.1864\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:01:54,866 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9349951124144672}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: backburn, TO: forum, PATH: backburn EAST technician SOUTH forum', 'FROM: forum, TO: dictaphone, PATH: forum WEST metabolite NORTH technician WEST backburn NORTH dictaphone', 'FROM: forum, TO: dictaphone, PATH: forum NORTH technician WEST backburn NORTH dictaphone', 'FROM: dictaphone, TO: technician, PATH: dictaphone SOUTH backburn SOUTH metabolite EAST forum', 'FROM: deliberation, TO: forum, PATH: deliberation EAST metabolite EAST forum']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:02:53 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:02:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-02-53_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:02:54 - INFO - datasets.builder - Using custom data configuration default-4cffebf296e1c72d\n",
      "02/12/2024 21:02:54 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:02:54 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-4cffebf296e1c72d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-4cffebf296e1c72d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 16384.00it/s]\n",
      "02/12/2024 21:02:54 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:02:54 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1019.89it/s]\n",
      "02/12/2024 21:02:54 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:02:54 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:02:54 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-4cffebf296e1c72d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2024.28it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:02:54,472 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:02:54,472 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:02:54,482 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:02:54,482 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:02:54,482 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:02:54,482 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:02:54,482 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:02:54,482 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:02:54,522 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:02:54,604 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:02:55,938 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:02:55,938 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:02:55,939 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:02:55,939 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/106 [00:00<?, ? examples/s]02/12/2024 21:02:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4cffebf296e1c72d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-aa04c5a1604c8625.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/106 [00:00<?, ? examples/s]02/12/2024 21:02:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4cffebf296e1c72d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-8d2a9f24dee5c548.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/106 [00:00<?, ? examples/s]02/12/2024 21:02:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4cffebf296e1c72d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-857b7fa180f7f30d.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/106 [00:00<?, ? examples/s]02/12/2024 21:02:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-4cffebf296e1c72d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-3c951f7504f0171e.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:02:57,155 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:02:57,156 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:02:57,156 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:02:57,156 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:02:57,156 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:02:57,156 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:02:57,156 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:02:57,156 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.37it/s][INFO|trainer.py:1912] 2024-02-12 21:03:00,528 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3.3741, 'train_samples_per_second': 1.185, 'train_steps_per_second': 1.185, 'train_loss': 0.5659515857696533, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.19it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:03:00,533 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:03:00,535 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:03:00,535 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:03:01,116 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:03:01,119 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:03:01,119 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =      0.566\n",
      "  train_runtime            = 0:00:03.37\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.185\n",
      "  train_steps_per_second   =      1.185\n",
      "02/12/2024 21:03:01 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:03:01,162 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:03:01,162 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:03:01,162 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 11.56it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.8954\n",
      "  eval_loss               =     0.5148\n",
      "  eval_runtime            = 0:00:00.32\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.099\n",
      "  eval_steps_per_second   =      6.099\n",
      "  perplexity              =     1.6734\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:03:01,739 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.895405669599218}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8a8647a57a4d6aba67048096a41c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb88639564a40f9af4f527d0543bf6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.6787928283214568], [0, 1, 4.904977554082871], [1, 0, 1.6949962705373764], [1, 1, 3.3475213170051576], [2, 0, 1.7425754994153977], [2, 1, 2.7078740179538725], [3, 0, 1.7362119734287262], [3, 1, 2.3355567395687102], [4, 0, 1.8724202185869216], [4, 1, 2.0651079058647155], [5, 0, 2.3775364249944686], [5, 1, 2.2084951758384705], [6, 0, 2.7467838525772095], [6, 1, 1.8725458562374115], [7, 0, 2.8972118735313415], [7, 1, 1.7609489500522613]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:03:13 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:03:13 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-03-13_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:03:14 - INFO - datasets.builder - Using custom data configuration default-139af6d6ca1d083f\n",
      "02/12/2024 21:03:14 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:03:14 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-139af6d6ca1d083f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-139af6d6ca1d083f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 16644.06it/s]\n",
      "02/12/2024 21:03:14 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:03:14 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2621.44it/s]\n",
      "02/12/2024 21:03:14 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:03:14 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:03:14 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-139af6d6ca1d083f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2087.76it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:03:14,115 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:03:14,115 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:03:14,124 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:03:14,124 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:03:14,124 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:03:14,124 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:03:14,124 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:03:14,124 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:03:14,158 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:03:14,230 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:03:15,543 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:03:15,543 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:03:15,544 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:03:15,544 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:03:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-139af6d6ca1d083f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-c27f18b471958aa4.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:03:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-139af6d6ca1d083f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f0f77390fdd2e852.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:03:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-139af6d6ca1d083f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-fc86711cead0a7e6.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:03:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-139af6d6ca1d083f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-c4e168e1c63a01b9.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:03:16,803 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:03:16,804 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:03:16,804 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:03:16,804 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:03:16,804 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:03:16,804 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:03:16,804 >>   Total optimization steps = 16\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:03:16,804 >>   Number of trainable parameters = 124,439,808\n",
      "100%|███████████████████████████████████████████| 16/16 [00:10<00:00,  1.77it/s][INFO|trainer.py:1912] 2024-02-12 21:03:27,149 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 10.3482, 'train_samples_per_second': 1.546, 'train_steps_per_second': 1.546, 'train_loss': 0.1972004920244217, 'epoch': 8.0}\n",
      "100%|███████████████████████████████████████████| 16/16 [00:10<00:00,  1.55it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:03:27,155 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:03:27,157 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:03:27,157 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:03:28,063 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:03:28,072 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:03:28,073 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.1972\n",
      "  train_runtime            = 0:00:10.34\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.546\n",
      "  train_steps_per_second   =      1.546\n",
      "02/12/2024 21:03:28 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:03:28,113 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:03:28,113 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:03:28,113 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 11.73it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =      0.937\n",
      "  eval_loss               =     0.1674\n",
      "  eval_runtime            = 0:00:00.36\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      5.447\n",
      "  eval_steps_per_second   =      5.447\n",
      "  perplexity              =     1.1823\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:03:28,776 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9369501466275659}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: technician, TO: forum, PATH: technician SOUTH forum', 'FROM: ovary, TO: grape, PATH: ovary NORTH nucleotidase EAST backburn NORTH grape', 'FROM: grape, TO: ovary, PATH: grape SOUTH backburn SOUTH metabolite WEST ovary', 'FROM: phone, TO: ovary, PATH: phone WEST grape WEST ovary', 'FROM: dictaphone, TO: ovary, PATH: dictaphone SOUTH nucleotidase SOUTH ovary']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:04:15 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:04:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-04-14_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:04:15 - INFO - datasets.builder - Using custom data configuration default-ed5894a537b4925a\n",
      "02/12/2024 21:04:15 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:04:15 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-ed5894a537b4925a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-ed5894a537b4925a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 17403.75it/s]\n",
      "02/12/2024 21:04:15 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:04:15 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1168.98it/s]\n",
      "02/12/2024 21:04:15 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:04:15 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:04:15 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-ed5894a537b4925a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2101.35it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:04:15,651 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:04:15,652 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:04:15,660 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:04:15,660 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:04:15,660 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:04:15,660 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:04:15,660 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:04:15,660 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:04:15,697 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:04:15,768 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:04:17,107 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:04:17,107 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:04:17,108 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:04:17,108 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/105 [00:00<?, ? examples/s]02/12/2024 21:04:17 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-ed5894a537b4925a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-54a863b174eaf0bf.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/105 [00:00<?, ? examples/s]02/12/2024 21:04:17 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-ed5894a537b4925a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-c315c5f9595c29da.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/105 [00:00<?, ? examples/s]02/12/2024 21:04:17 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-ed5894a537b4925a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-16183ab16bf197fd.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/105 [00:00<?, ? examples/s]02/12/2024 21:04:17 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-ed5894a537b4925a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-9499f7fe08c76795.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:04:18,562 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:04:18,562 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:04:18,562 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:04:18,562 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:04:18,562 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:04:18,562 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:04:18,562 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:04:18,563 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.35it/s][INFO|trainer.py:1912] 2024-02-12 21:04:21,952 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3.3908, 'train_samples_per_second': 1.18, 'train_steps_per_second': 1.18, 'train_loss': 0.5066927075386047, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.18it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:04:21,962 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:04:21,963 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:04:21,963 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:04:22,517 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:04:22,521 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:04:22,521 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.5067\n",
      "  train_runtime            = 0:00:03.39\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =       1.18\n",
      "  train_steps_per_second   =       1.18\n",
      "02/12/2024 21:04:22 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:04:22,550 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:04:22,550 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:04:22,550 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 13.46it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.8895\n",
      "  eval_loss               =     0.4305\n",
      "  eval_runtime            = 0:00:00.30\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.602\n",
      "  eval_steps_per_second   =      6.602\n",
      "  perplexity              =      1.538\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:04:23,137 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8895405669599218}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca257307004f4a43954533b577273d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3dfa7481fc4f888c8517cd70d1bb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.6787928283214568], [0, 1, 4.904977554082871], [1, 0, 1.6949962705373764], [1, 1, 3.3475213170051576], [2, 0, 1.7425754994153977], [2, 1, 2.7078740179538725], [3, 0, 1.7362119734287262], [3, 1, 2.3355567395687102], [4, 0, 1.8724202185869216], [4, 1, 2.0651079058647155], [5, 0, 2.3775364249944686], [5, 1, 2.2084951758384705], [6, 0, 2.7467838525772095], [6, 1, 1.8725458562374115], [7, 0, 2.8972118735313415], [7, 1, 1.7609489500522613], [8, 0, 3.3041542410850524], [8, 1, 1.6425170034170151]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:04:34 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:04:34 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-04-34_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:04:34 - INFO - datasets.builder - Using custom data configuration default-e65f9c1689c86916\n",
      "02/12/2024 21:04:34 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:04:34 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-e65f9c1689c86916/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-e65f9c1689c86916/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 18157.16it/s]\n",
      "02/12/2024 21:04:34 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:04:34 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2673.23it/s]\n",
      "02/12/2024 21:04:34 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:04:34 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:04:34 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-e65f9c1689c86916/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2125.31it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:04:34,944 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:04:34,945 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:04:34,953 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:04:34,953 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:04:34,953 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:04:34,953 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:04:34,953 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:04:34,953 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:04:34,987 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:04:35,055 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:04:36,367 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:04:36,367 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:04:36,368 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:04:36,368 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:04:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e65f9c1689c86916/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-e0c6c73241d16ed0.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:04:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e65f9c1689c86916/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-5e47b0456f2f228f.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:04:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e65f9c1689c86916/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f18bce9620f6107e.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:04:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e65f9c1689c86916/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-0cf6fd6ef49e5124.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:04:37,651 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:04:37,651 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:04:37,651 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:04:37,651 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:04:37,651 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:04:37,651 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:04:37,651 >>   Total optimization steps = 16\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:04:37,651 >>   Number of trainable parameters = 124,439,808\n",
      "100%|███████████████████████████████████████████| 16/16 [00:10<00:00,  1.74it/s][INFO|trainer.py:1912] 2024-02-12 21:04:47,677 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 10.0269, 'train_samples_per_second': 1.596, 'train_steps_per_second': 1.596, 'train_loss': 0.19803380966186523, 'epoch': 8.0}\n",
      "100%|███████████████████████████████████████████| 16/16 [00:10<00:00,  1.60it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:04:47,681 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:04:47,683 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:04:47,684 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:04:48,302 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:04:48,305 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:04:48,306 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =      0.198\n",
      "  train_runtime            = 0:00:10.02\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.596\n",
      "  train_steps_per_second   =      1.596\n",
      "02/12/2024 21:04:48 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:04:48,336 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:04:48,336 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:04:48,336 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 13.31it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =      0.936\n",
      "  eval_loss               =     0.1688\n",
      "  eval_runtime            = 0:00:00.30\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.566\n",
      "  eval_steps_per_second   =      6.566\n",
      "  perplexity              =     1.1839\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:04:49,022 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9359726295210166}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: metabolite, TO: forum, PATH: metabolite EAST forum', 'FROM: forum, TO: grape, PATH: forum WEST metabolite NORTH backburn NORTH grape', 'FROM: forum, TO: forum, PATH: forum', 'FROM: technician, TO: forum, PATH: technician SOUTH forum', 'FROM: backburn, TO: forum, PATH: backburn SOUTH metabolite EAST forum']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:05:39 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:05:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-05-39_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:05:39 - INFO - datasets.builder - Using custom data configuration default-473bd17b2c4252cb\n",
      "02/12/2024 21:05:40 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:05:40 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-473bd17b2c4252cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-473bd17b2c4252cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 17403.75it/s]\n",
      "02/12/2024 21:05:40 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:05:40 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2634.61it/s]\n",
      "02/12/2024 21:05:40 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:05:40 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:05:40 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-473bd17b2c4252cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2111.94it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:05:40,032 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:05:40,032 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:05:40,041 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:05:40,041 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:05:40,041 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:05:40,041 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:05:40,041 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:05:40,041 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:05:40,074 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:05:40,142 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:05:41,476 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:05:41,476 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:05:41,477 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:05:41,477 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/107 [00:00<?, ? examples/s]02/12/2024 21:05:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-473bd17b2c4252cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f72eba30dc5f0b10.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/107 [00:00<?, ? examples/s]02/12/2024 21:05:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-473bd17b2c4252cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-246ffdf5db69e664.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/107 [00:00<?, ? examples/s]02/12/2024 21:05:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-473bd17b2c4252cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-e0d57a9ea88690a1.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/107 [00:00<?, ? examples/s]02/12/2024 21:05:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-473bd17b2c4252cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-e0319211b20fe6cd.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:05:43,253 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:05:43,253 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:05:43,253 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:05:43,253 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:05:43,253 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:05:43,253 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:05:43,253 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:05:43,254 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.38it/s][INFO|trainer.py:1912] 2024-02-12 21:05:46,580 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3.3275, 'train_samples_per_second': 1.202, 'train_steps_per_second': 1.202, 'train_loss': 0.4264095425605774, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.20it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:05:46,584 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:05:46,585 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:05:46,586 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:05:47,131 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:05:47,132 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:05:47,132 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.4264\n",
      "  train_runtime            = 0:00:03.32\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.202\n",
      "  train_steps_per_second   =      1.202\n",
      "02/12/2024 21:05:47 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:05:47,159 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:05:47,159 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:05:47,159 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 13.22it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.8954\n",
      "  eval_loss               =     0.3551\n",
      "  eval_runtime            = 0:00:00.29\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.672\n",
      "  eval_steps_per_second   =      6.672\n",
      "  perplexity              =     1.4263\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:05:47,739 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.895405669599218}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d676d619bb6147cd98ae146075209a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eed91f2820b4359bd1959b6438344e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.6787928283214568], [0, 1, 4.904977554082871], [1, 0, 1.6949962705373764], [1, 1, 3.3475213170051576], [2, 0, 1.7425754994153977], [2, 1, 2.7078740179538725], [3, 0, 1.7362119734287262], [3, 1, 2.3355567395687102], [4, 0, 1.8724202185869216], [4, 1, 2.0651079058647155], [5, 0, 2.3775364249944686], [5, 1, 2.2084951758384705], [6, 0, 2.7467838525772095], [6, 1, 1.8725458562374115], [7, 0, 2.8972118735313415], [7, 1, 1.7609489500522613], [8, 0, 3.3041542410850524], [8, 1, 1.6425170034170151], [9, 0, 3.6089135706424713], [9, 1, 1.7191916018724442]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: boom, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: championship, Predicted location: championship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: apartment, Predicted location: apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: championship, Predicted location: championship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: boom, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: extent, Predicted location: extent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: boom, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: apartment, Predicted location: apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: championship, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: ovary, Predicted location: jet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: extent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: extent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: forum, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: dictaphone, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: jet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: extent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: forum, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: extent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: dictaphone, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grasshopper, Predicted location: apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: ovary, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: extent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: pin\n",
      "Correct location: metabolite, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: backburn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: boom, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: championship, Predicted location: forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: apartment, Predicted location: apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: backburn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: championship, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: boom, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: backburn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: extent, Predicted location: extent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: boom, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: backburn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: apartment, Predicted location: nucleotidase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: championship, Predicted location: forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: backburn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: ovary, Predicted location: ovary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: metabolite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: backburn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: nucleotidase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: forum, Predicted location: forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: dictaphone, Predicted location: dictaphone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: metabolite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: forum, Predicted location: forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: backburn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: dictaphone, Predicted location: dictaphone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: backburn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grasshopper, Predicted location: grasshopper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: nucleotidase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: nucleotidase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: nucleotidase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: backburn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: metabolite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: ovary, Predicted location: ovary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: nucleotidase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: metabolite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: grape\n",
      "Correct location: metabolite, Predicted location: grape\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[([[0, 0, 0.9], [0, 1, 0.0], [1, 0, 0.5], [1, 1, 0.85]],\n",
       "  [[0, 0, 1.6787928283214568],\n",
       "   [0, 1, 4.904977554082871],\n",
       "   [1, 0, 1.6949962705373764],\n",
       "   [1, 1, 3.3475213170051576],\n",
       "   [2, 0, 1.7425754994153977],\n",
       "   [2, 1, 2.7078740179538725],\n",
       "   [3, 0, 1.7362119734287262],\n",
       "   [3, 1, 2.3355567395687102],\n",
       "   [4, 0, 1.8724202185869216],\n",
       "   [4, 1, 2.0651079058647155],\n",
       "   [5, 0, 2.3775364249944686],\n",
       "   [5, 1, 2.2084951758384705],\n",
       "   [6, 0, 2.7467838525772095],\n",
       "   [6, 1, 1.8725458562374115],\n",
       "   [7, 0, 2.8972118735313415],\n",
       "   [7, 1, 1.7609489500522613],\n",
       "   [8, 0, 3.3041542410850524],\n",
       "   [8, 1, 1.6425170034170151],\n",
       "   [9, 0, 3.6089135706424713],\n",
       "   [9, 1, 1.7191916018724442]])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_schedule_multiple_seeds(seeds=[0,1,2], \n",
    "                                   total_eps=100, \n",
    "                                   num_cycles=10, \n",
    "                                   start_fraction_rem=0.2, \n",
    "                                   end_fraction_rem=0.2,\n",
    "                                   num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38b87f6d-7722-4e26-bedf-37deef5f7bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to train model from base_model_b8 and save to spatial_model_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:14:45 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:14:45 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_0/runs/Feb12_21-14-45_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=20.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_0,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_0,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:14:46 - INFO - datasets.builder - Using custom data configuration default-5c8348809d7d99e4\n",
      "02/12/2024 21:14:46 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:14:46 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-5c8348809d7d99e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-5c8348809d7d99e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 16946.68it/s]\n",
      "02/12/2024 21:14:46 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:14:46 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1526.87it/s]\n",
      "02/12/2024 21:14:46 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:14:46 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:14:46 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-5c8348809d7d99e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2070.24it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:14:46,628 >> loading configuration file base_model_b8/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:14:46,629 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"base_model_b8\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:14:46,637 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:14:46,637 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:14:46,637 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:14:46,637 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:14:46,637 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:14:46,637 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:14:46,671 >> loading weights file base_model_b8/model.safetensors\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:14:46,686 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:14:48,669 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:14:48,669 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at base_model_b8.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:14:48,670 >> loading configuration file base_model_b8/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:14:48,670 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|             | 0/1000 [00:00<?, ? examples/s]02/12/2024 21:14:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-5c8348809d7d99e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-454c422dccda4a3e.arrow\n",
      "Running tokenizer on dataset:   0%|             | 0/1000 [00:00<?, ? examples/s]02/12/2024 21:14:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-5c8348809d7d99e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-89468f938047ed2f.arrow\n",
      "Grouping texts in chunks of 1024:   0%|         | 0/1000 [00:00<?, ? examples/s]02/12/2024 21:14:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-5c8348809d7d99e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-6ff1c27462013d4f.arrow\n",
      "Grouping texts in chunks of 1024:   0%|         | 0/1000 [00:00<?, ? examples/s]02/12/2024 21:14:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-5c8348809d7d99e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-95eb1a9eec94d394.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:14:50,130 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:14:50,130 >>   Num examples = 20\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:14:50,130 >>   Num Epochs = 20\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:14:50,130 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:14:50,130 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:14:50,130 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:14:50,130 >>   Total optimization steps = 400\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:14:50,130 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████| 400/400 [03:52<00:00,  1.70it/s][INFO|trainer.py:1912] 2024-02-12 21:18:42,990 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 232.861, 'train_samples_per_second': 1.718, 'train_steps_per_second': 1.718, 'train_loss': 0.24003999710083007, 'epoch': 20.0}\n",
      "100%|█████████████████████████████████████████| 400/400 [03:52<00:00,  1.72it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:18:43,007 >> Saving model checkpoint to spatial_model_0\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:18:43,010 >> Configuration saved in spatial_model_0/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:18:43,011 >> Configuration saved in spatial_model_0/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:18:43,627 >> Model weights saved in spatial_model_0/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:18:43,635 >> tokenizer config file saved in spatial_model_0/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:18:43,636 >> Special tokens file saved in spatial_model_0/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       20.0\n",
      "  train_loss               =       0.24\n",
      "  train_runtime            = 0:03:52.86\n",
      "  train_samples            =         20\n",
      "  train_samples_per_second =      1.718\n",
      "  train_steps_per_second   =      1.718\n",
      "02/12/2024 21:18:43 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:18:43,684 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:18:43,684 >>   Num examples = 20\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:18:43,684 >>   Batch size = 1\n",
      "100%|███████████████████████████████████████████| 20/20 [00:02<00:00,  7.12it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       20.0\n",
      "  eval_accuracy           =     0.9268\n",
      "  eval_loss               =     0.1913\n",
      "  eval_runtime            = 0:00:02.98\n",
      "  eval_samples            =         20\n",
      "  eval_samples_per_second =      6.698\n",
      "  eval_steps_per_second   =      6.698\n",
      "  perplexity              =     1.2108\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:18:47,408 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9268328445747801}]}\n",
      "NREM phase\n",
      "About to train model from spatial_model_0 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:18:54 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:18:54 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-18-53_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:18:54 - INFO - datasets.builder - Using custom data configuration default-833c1afca3b052e4\n",
      "02/12/2024 21:18:54 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:18:54 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-833c1afca3b052e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-833c1afca3b052e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 17886.16it/s]\n",
      "02/12/2024 21:18:54 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:18:54 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1143.49it/s]\n",
      "02/12/2024 21:18:54 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:18:54 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:18:54 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-833c1afca3b052e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1550.57it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:18:54,624 >> loading configuration file spatial_model_0/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:18:54,624 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_0\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:18:54,635 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:18:54,635 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:18:54,635 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:18:54,635 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:18:54,635 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:18:54,635 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:18:54,674 >> loading weights file spatial_model_0/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:18:54,772 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:18:56,112 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:18:56,112 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:18:56,112 >> loading configuration file spatial_model_0/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:18:56,113 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:18:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-833c1afca3b052e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-22cfa962a71adec7.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:18:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-833c1afca3b052e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-546611a316630368.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:18:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-833c1afca3b052e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-24aea93d44b317ff.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:18:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-833c1afca3b052e4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-d714cebe3db73e53.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:18:57,766 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:18:57,766 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:18:57,766 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:18:57,766 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:18:57,767 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:18:57,767 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:18:57,767 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:18:57,767 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.39it/s][INFO|trainer.py:1912] 2024-02-12 21:19:00,996 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3.2305, 'train_samples_per_second': 1.238, 'train_steps_per_second': 1.238, 'train_loss': 0.5115203857421875, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:19:01,003 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:19:01,004 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:19:01,005 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:19:01,597 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:19:01,599 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:19:01,599 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.5115\n",
      "  train_runtime            = 0:00:03.23\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.238\n",
      "  train_steps_per_second   =      1.238\n",
      "02/12/2024 21:19:01 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:19:01,629 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:19:01,629 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:19:01,629 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 13.43it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.9066\n",
      "  eval_loss               =     0.4186\n",
      "  eval_runtime            = 0:00:00.30\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.593\n",
      "  eval_steps_per_second   =      6.593\n",
      "  perplexity              =     1.5198\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:19:02,085 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9066471163245357}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: championship, TO: pioneer, PATH: championship WEST boom WEST extent SOUTH pin SOUTH pioneer', 'FROM: pin, TO: championship, PATH: pin EAST pioneer EAST signature NORTH championship', 'FROM: pioneer, TO: extent, PATH: pioneer WEST pilaf NORTH pin NORTH extent', 'FROM: signature, TO: boom, PATH: signature WEST pioneer NORTH pioneer NORTH boom', 'FROM: signature, TO: boom, PATH: signature NORTH championship WEST boom']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:19:45 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:19:45 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-19-45_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:19:46 - INFO - datasets.builder - Using custom data configuration default-db159f416f1b752c\n",
      "02/12/2024 21:19:46 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:19:46 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-db159f416f1b752c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-db159f416f1b752c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 17660.23it/s]\n",
      "02/12/2024 21:19:46 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:19:46 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2583.49it/s]\n",
      "02/12/2024 21:19:46 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:19:46 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:19:46 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-db159f416f1b752c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1964.55it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:19:46,281 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:19:46,282 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:19:46,291 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:19:46,291 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:19:46,291 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:19:46,291 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:19:46,291 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:19:46,291 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:19:46,329 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:19:46,403 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:19:47,739 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:19:47,739 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:19:47,739 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:19:47,740 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:19:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-db159f416f1b752c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-c4a9ccb526bfa715.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:19:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-db159f416f1b752c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-567eda81fbf11363.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:19:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-db159f416f1b752c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-6eeb8369505db653.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:19:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-db159f416f1b752c/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-e149de416e781043.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:19:49,057 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:19:49,057 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:19:49,057 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:19:49,057 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:19:49,057 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:19:49,057 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:19:49,057 >>   Total optimization steps = 8\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:19:49,057 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.63it/s][INFO|trainer.py:1912] 2024-02-12 21:19:54,760 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5.7047, 'train_samples_per_second': 1.402, 'train_steps_per_second': 1.402, 'train_loss': 0.5377086400985718, 'epoch': 8.0}\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.40it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:19:54,766 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:19:54,767 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:19:54,768 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:19:55,396 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:19:55,398 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:19:55,398 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.5377\n",
      "  train_runtime            = 0:00:05.70\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      1.402\n",
      "  train_steps_per_second   =      1.402\n",
      "02/12/2024 21:19:55 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:19:55,430 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:19:55,430 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:19:55,430 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 98.48it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.8974\n",
      "  eval_loss               =     0.3917\n",
      "  eval_runtime            = 0:00:00.16\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =      5.899\n",
      "  eval_steps_per_second   =      5.899\n",
      "  perplexity              =     1.4795\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:19:55,762 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8973607038123167}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e61366dd8ae488e8a3e50e9d90fd735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d94628915048f0868b9d53ca158cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.7017171770334243], [0, 1, 7.65284811258316]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:20:07 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:20:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-20-06_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:20:07 - INFO - datasets.builder - Using custom data configuration default-693f77ae0390b79a\n",
      "02/12/2024 21:20:07 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:20:07 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-693f77ae0390b79a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-693f77ae0390b79a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 16008.79it/s]\n",
      "02/12/2024 21:20:07 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:20:07 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2361.66it/s]\n",
      "02/12/2024 21:20:07 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:20:07 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:20:07 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-693f77ae0390b79a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1935.98it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:20:07,605 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:20:07,606 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:20:07,615 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:20:07,615 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:20:07,615 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:20:07,615 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:20:07,615 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:20:07,615 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:20:07,651 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:20:07,721 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:20:09,033 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:20:09,033 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:20:09,034 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:20:09,034 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:20:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-693f77ae0390b79a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-15d84d7769768654.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:20:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-693f77ae0390b79a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-ac1ebabb704c01fe.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:20:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-693f77ae0390b79a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-4e1eb9af8155e1bf.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:20:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-693f77ae0390b79a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-3fd696e11482dd34.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:20:10,452 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:20:10,452 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:20:10,452 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:20:10,452 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:20:10,452 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:20:10,452 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:20:10,452 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:20:10,453 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.39it/s][INFO|trainer.py:1912] 2024-02-12 21:20:13,633 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3.1818, 'train_samples_per_second': 1.257, 'train_steps_per_second': 1.257, 'train_loss': 0.43917837738990784, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.26it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:20:13,637 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:20:13,638 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:20:13,638 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:20:14,180 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:20:14,181 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:20:14,181 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.4392\n",
      "  train_runtime            = 0:00:03.18\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.257\n",
      "  train_steps_per_second   =      1.257\n",
      "02/12/2024 21:20:14 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:20:14,209 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:20:14,210 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:20:14,210 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 13.37it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.9101\n",
      "  eval_loss               =     0.3565\n",
      "  eval_runtime            = 0:00:00.30\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =       6.65\n",
      "  eval_steps_per_second   =       6.65\n",
      "  perplexity              =     1.4283\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:20:14,769 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9100684261974584}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: apartment, TO: pioneer, PATH: apartment NORTH pioneer', 'FROM: signature, TO: boom, PATH: signature WEST pioneer NORTH boom', 'FROM: pioneer, TO: jet, PATH: pioneer SOUTH pilaf WEST jet', 'FROM: extent, TO: signature, PATH: extent EAST boom EAST championship SOUTH signature', 'FROM: championship, TO: pioneer, PATH: championship SOUTH signature WEST pioneer']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:20:58 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:20:58 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-20-58_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:20:59 - INFO - datasets.builder - Using custom data configuration default-54976afddb06a0cb\n",
      "02/12/2024 21:20:59 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:20:59 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-54976afddb06a0cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-54976afddb06a0cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 17549.39it/s]\n",
      "02/12/2024 21:20:59 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:20:59 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1098.13it/s]\n",
      "02/12/2024 21:20:59 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:20:59 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:20:59 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-54976afddb06a0cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2034.59it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:20:59,063 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:20:59,064 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:20:59,073 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:20:59,073 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:20:59,073 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:20:59,073 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:20:59,073 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:20:59,073 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:20:59,114 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:20:59,186 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:21:00,531 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:21:00,531 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:21:00,531 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:21:00,532 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/103 [00:00<?, ? examples/s]02/12/2024 21:21:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-54976afddb06a0cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-830a35ee32dcd6b6.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/103 [00:00<?, ? examples/s]02/12/2024 21:21:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-54976afddb06a0cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-d737f6e87bc9734f.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/103 [00:00<?, ? examples/s]02/12/2024 21:21:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-54976afddb06a0cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-20ddfd4f0765825f.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/103 [00:00<?, ? examples/s]02/12/2024 21:21:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-54976afddb06a0cb/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-fc2be4591b5890df.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:21:01,817 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:21:01,817 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:21:01,817 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:21:01,818 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:21:01,818 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:21:01,818 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:21:01,818 >>   Total optimization steps = 8\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:21:01,818 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.67it/s][INFO|trainer.py:1912] 2024-02-12 21:21:07,513 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5.6967, 'train_samples_per_second': 1.404, 'train_steps_per_second': 1.404, 'train_loss': 0.5627965927124023, 'epoch': 8.0}\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.40it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:21:07,518 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:21:07,519 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:21:07,519 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:21:08,098 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:21:08,098 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:21:08,098 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.5628\n",
      "  train_runtime            = 0:00:05.69\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      1.404\n",
      "  train_steps_per_second   =      1.404\n",
      "02/12/2024 21:21:08 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:21:08,126 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:21:08,126 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:21:08,126 >>   Batch size = 1\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 114.78it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =      0.869\n",
      "  eval_loss               =     0.4206\n",
      "  eval_runtime            = 0:00:00.15\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =      6.363\n",
      "  eval_steps_per_second   =      6.363\n",
      "  perplexity              =     1.5229\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:21:08,413 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8690127077223851}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a296d7c59b94784ad7d5037ed5eb065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6209af023814a2a82e8321bb39e3743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.7017171770334243], [0, 1, 7.65284811258316], [1, 0, 1.7324846714735032], [1, 1, 5.543031466007233]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:21:20 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:21:20 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-21-19_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:21:21 - INFO - datasets.builder - Using custom data configuration default-6cbab2d10b08af04\n",
      "02/12/2024 21:21:21 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:21:21 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-6cbab2d10b08af04/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-6cbab2d10b08af04/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 17549.39it/s]\n",
      "02/12/2024 21:21:21 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:21:21 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 3148.88it/s]\n",
      "02/12/2024 21:21:21 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:21:21 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:21:21 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-6cbab2d10b08af04/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2385.84it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:21:21,391 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:21:21,392 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:21:21,399 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:21:21,399 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:21:21,399 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:21:21,399 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:21:21,399 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:21:21,400 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:21:21,438 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:21:21,510 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:21:22,943 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:21:22,943 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:21:22,944 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:21:22,944 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:21:22 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-6cbab2d10b08af04/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-6ea5e6784ab5f458.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:21:22 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-6cbab2d10b08af04/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-d0ec07c6842f4318.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:21:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-6cbab2d10b08af04/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-1b6878447095ac13.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:21:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-6cbab2d10b08af04/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-8508d684974026fe.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:21:24,779 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:21:24,779 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:21:24,779 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:21:24,779 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:21:24,779 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:21:24,779 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:21:24,779 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:21:24,779 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.04it/s][INFO|trainer.py:1912] 2024-02-12 21:21:29,142 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 4.3641, 'train_samples_per_second': 0.917, 'train_steps_per_second': 0.917, 'train_loss': 0.3868604302406311, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.09s/it]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:21:29,151 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:21:29,152 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:21:29,153 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:21:29,877 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:21:29,878 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:21:29,878 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.3869\n",
      "  train_runtime            = 0:00:04.36\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      0.917\n",
      "  train_steps_per_second   =      0.917\n",
      "02/12/2024 21:21:29 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:21:29,913 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:21:29,913 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:21:29,913 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  8.81it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =      0.911\n",
      "  eval_loss               =     0.3384\n",
      "  eval_runtime            = 0:00:00.41\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =       4.85\n",
      "  eval_steps_per_second   =       4.85\n",
      "  perplexity              =     1.4027\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:21:30,469 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9110459433040078}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: jet, TO: championship, PATH: jet NORTH pin NORTH pioneer EAST championship', 'FROM: boom, TO: pin, PATH: boom WEST pioneer SOUTH pin', 'FROM: pin, TO: championship, PATH: pin EAST pioneer NORTH championship', 'FROM: championship, TO: pioneer, PATH: championship WEST championship SOUTH pioneer', 'FROM: championship, TO: pin, PATH: championship WEST boom SOUTH pioneer WEST pin']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:22:18 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:22:18 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-22-17_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:22:18 - INFO - datasets.builder - Using custom data configuration default-e45bafa55622dc03\n",
      "02/12/2024 21:22:18 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:22:18 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-e45bafa55622dc03/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-e45bafa55622dc03/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 16810.84it/s]\n",
      "02/12/2024 21:22:18 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:22:18 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2621.44it/s]\n",
      "02/12/2024 21:22:18 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:22:18 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:22:18 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-e45bafa55622dc03/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2161.46it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:22:18,690 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:22:18,690 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:22:18,699 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:22:18,699 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:22:18,699 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:22:18,699 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:22:18,699 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:22:18,699 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:22:18,732 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:22:18,801 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:22:20,122 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:22:20,123 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:22:20,123 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:22:20,123 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/103 [00:00<?, ? examples/s]02/12/2024 21:22:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e45bafa55622dc03/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-2956a61b0af68eeb.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/103 [00:00<?, ? examples/s]02/12/2024 21:22:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e45bafa55622dc03/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-6adcae74a2a3f6ba.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/103 [00:00<?, ? examples/s]02/12/2024 21:22:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e45bafa55622dc03/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-e30e1587a51e3c5b.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/103 [00:00<?, ? examples/s]02/12/2024 21:22:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e45bafa55622dc03/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-ea7194dbdd430243.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:22:21,417 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:22:21,417 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:22:21,417 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:22:21,417 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:22:21,417 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:22:21,417 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:22:21,417 >>   Total optimization steps = 8\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:22:21,418 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.58it/s][INFO|trainer.py:1912] 2024-02-12 21:22:27,381 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5.9678, 'train_samples_per_second': 1.341, 'train_steps_per_second': 1.341, 'train_loss': 0.5228341817855835, 'epoch': 8.0}\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.34it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:22:27,391 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:22:27,393 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:22:27,393 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:22:28,016 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:22:28,021 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:22:28,021 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.5228\n",
      "  train_runtime            = 0:00:05.96\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      1.341\n",
      "  train_steps_per_second   =      1.341\n",
      "02/12/2024 21:22:28 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:22:28,061 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:22:28,061 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:22:28,062 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 33.42it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.8719\n",
      "  eval_loss               =     0.4038\n",
      "  eval_runtime            = 0:00:00.19\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =      5.226\n",
      "  eval_steps_per_second   =      5.226\n",
      "  perplexity              =     1.4976\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:22:28,590 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8719452590420332}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f276360aa94b5fb4c9f694c0a31d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed342ebae3364c8696f234e43e094b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.7017171770334243], [0, 1, 7.65284811258316], [1, 0, 1.7324846714735032], [1, 1, 5.543031466007233], [2, 0, 1.7540715426206588], [2, 1, 5.026992207765579]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:22:39 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:22:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-22-39_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:22:40 - INFO - datasets.builder - Using custom data configuration default-d2c28601bc4c1b4b\n",
      "02/12/2024 21:22:40 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:22:40 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-d2c28601bc4c1b4b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-d2c28601bc4c1b4b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 15141.89it/s]\n",
      "02/12/2024 21:22:40 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:22:40 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2193.67it/s]\n",
      "02/12/2024 21:22:40 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:22:40 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:22:40 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-d2c28601bc4c1b4b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1754.20it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:22:40,462 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:22:40,462 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:22:40,471 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:22:40,471 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:22:40,471 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:22:40,471 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:22:40,471 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:22:40,471 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:22:40,506 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:22:40,576 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:22:41,897 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:22:41,897 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:22:41,898 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:22:41,898 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:22:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d2c28601bc4c1b4b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-8201b47d73100dbf.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:22:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d2c28601bc4c1b4b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-b78b3fab31e962e6.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:22:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d2c28601bc4c1b4b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-6c052a92382b1202.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:22:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-d2c28601bc4c1b4b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f0fe8ecaa1c82aee.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:22:43,218 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:22:43,218 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:22:43,218 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:22:43,218 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:22:43,218 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:22:43,218 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:22:43,218 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:22:43,218 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.30it/s][INFO|trainer.py:1912] 2024-02-12 21:22:46,523 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3.3101, 'train_samples_per_second': 1.208, 'train_steps_per_second': 1.208, 'train_loss': 0.364490270614624, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.21it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:22:46,531 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:22:46,532 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:22:46,533 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:22:47,141 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:22:47,141 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:22:47,142 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.3645\n",
      "  train_runtime            = 0:00:03.31\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.208\n",
      "  train_steps_per_second   =      1.208\n",
      "02/12/2024 21:22:47 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:22:47,181 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:22:47,181 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:22:47,181 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 11.26it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =      0.915\n",
      "  eval_loss               =     0.2986\n",
      "  eval_runtime            = 0:00:00.36\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      5.463\n",
      "  eval_steps_per_second   =      5.463\n",
      "  perplexity              =     1.3479\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:22:47,729 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9149560117302052}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: pilaf, TO: championship, PATH: pilaf NORTH pioneer EAST signature EAST championship', 'FROM: championship, TO: pioneer, PATH: championship WEST championship SOUTH signature SOUTH pioneer', 'FROM: pioneer, TO: championship, PATH: pioneer EAST signature EAST championship', 'FROM: pioneer, TO: championship, PATH: pioneer NORTH championship', 'FROM: extent, TO: signature, PATH: extent WEST pioneer SOUTH pilaf WEST jet NORTH pioneer']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:23:35 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:23:35 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-23-35_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:23:36 - INFO - datasets.builder - Using custom data configuration default-e08165410adc06e8\n",
      "02/12/2024 21:23:36 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:23:36 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-e08165410adc06e8/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-e08165410adc06e8/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 23237.14it/s]\n",
      "02/12/2024 21:23:36 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:23:36 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 3306.51it/s]\n",
      "02/12/2024 21:23:36 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:23:36 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:23:36 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-e08165410adc06e8/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1196.15it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:23:36,362 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:23:36,363 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:23:36,372 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:23:36,372 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:23:36,372 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:23:36,372 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:23:36,372 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:23:36,372 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:23:36,411 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:23:36,491 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:23:37,926 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:23:37,926 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:23:37,927 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:23:37,927 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/101 [00:00<?, ? examples/s]02/12/2024 21:23:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e08165410adc06e8/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-88239f2094927aaa.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/101 [00:00<?, ? examples/s]02/12/2024 21:23:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e08165410adc06e8/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-82a99d816d7e6aae.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/101 [00:00<?, ? examples/s]02/12/2024 21:23:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e08165410adc06e8/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-01aa2418ca80ba1f.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/101 [00:00<?, ? examples/s]02/12/2024 21:23:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e08165410adc06e8/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-2fe36e62119c19c2.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:23:39,192 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:23:39,192 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:23:39,192 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:23:39,192 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:23:39,192 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:23:39,192 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:23:39,192 >>   Total optimization steps = 8\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:23:39,192 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.63it/s][INFO|trainer.py:1912] 2024-02-12 21:23:45,169 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5.9813, 'train_samples_per_second': 1.338, 'train_steps_per_second': 1.338, 'train_loss': 0.5442614555358887, 'epoch': 8.0}\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.34it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:23:45,177 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:23:45,178 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:23:45,178 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:23:45,813 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:23:45,813 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:23:45,813 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.5443\n",
      "  train_runtime            = 0:00:05.98\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      1.338\n",
      "  train_steps_per_second   =      1.338\n",
      "02/12/2024 21:23:45 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:23:45,844 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:23:45,844 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:23:45,844 >>   Batch size = 1\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 113.59it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.8631\n",
      "  eval_loss               =     0.4051\n",
      "  eval_runtime            = 0:00:00.16\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =      5.967\n",
      "  eval_steps_per_second   =      5.967\n",
      "  perplexity              =     1.4994\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:23:46,212 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.863147605083089}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cd15d2e1764615a09fe4bb3afc4259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f51991bbd9409a8476fc5940ba256f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.7017171770334243], [0, 1, 7.65284811258316], [1, 0, 1.7324846714735032], [1, 1, 5.543031466007233], [2, 0, 1.7540715426206588], [2, 1, 5.026992207765579], [3, 0, 1.780388554930687], [3, 1, 4.529018920660019]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:23:57 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:23:57 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-23-57_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:23:57 - INFO - datasets.builder - Using custom data configuration default-ac5c0a2d2d151af3\n",
      "02/12/2024 21:23:57 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:23:57 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-ac5c0a2d2d151af3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-ac5c0a2d2d151af3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 17586.18it/s]\n",
      "02/12/2024 21:23:57 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:23:57 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2541.23it/s]\n",
      "02/12/2024 21:23:57 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:23:57 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:23:57 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-ac5c0a2d2d151af3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2132.34it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:23:57,952 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:23:57,953 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:23:57,961 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:23:57,961 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:23:57,961 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:23:57,961 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:23:57,961 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:23:57,961 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:23:57,995 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:23:58,063 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:23:59,374 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:23:59,375 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:23:59,375 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:23:59,375 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:23:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-ac5c0a2d2d151af3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f6601b29553c4e6b.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:23:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-ac5c0a2d2d151af3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-952b6eb5121949a2.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:23:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-ac5c0a2d2d151af3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-1b5e0482eeb4456c.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:23:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-ac5c0a2d2d151af3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-96c6cb67263c4fba.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:24:00,666 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:24:00,666 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:24:00,666 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:24:00,666 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:24:00,666 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:24:00,666 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:24:00,666 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:24:00,666 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.41it/s][INFO|trainer.py:1912] 2024-02-12 21:24:03,849 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3.1837, 'train_samples_per_second': 1.256, 'train_steps_per_second': 1.256, 'train_loss': 0.315374493598938, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.26it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:24:03,853 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:24:03,853 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:24:03,854 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:24:04,412 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:24:04,412 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:24:04,412 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.3154\n",
      "  train_runtime            = 0:00:03.18\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.256\n",
      "  train_steps_per_second   =      1.256\n",
      "02/12/2024 21:24:04 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:24:04,439 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:24:04,439 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:24:04,439 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 13.55it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.9213\n",
      "  eval_loss               =     0.2663\n",
      "  eval_runtime            = 0:00:00.29\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.692\n",
      "  eval_steps_per_second   =      6.692\n",
      "  perplexity              =     1.3051\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:24:05,044 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9213098729227761}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: jet, TO: championship, PATH: jet NORTH pin NORTH pioneer EAST signature EAST championship', 'FROM: jet, TO: signature, PATH: jet WEST championship WEST pioneer NORTH pilaf NORTH signature', 'FROM: signature, TO: pioneer, PATH: signature SOUTH pioneer', 'FROM: pin, TO: pilaf, PATH: pin NORTH pioneer WEST pilaf', 'FROM: pendant, TO: pioneer, PATH: pendant NORTH signature WEST boom WEST pioneer']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:24:48 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:24:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-24-48_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:24:49 - INFO - datasets.builder - Using custom data configuration default-5f7c83e0929a7073\n",
      "02/12/2024 21:24:49 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:24:49 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-5f7c83e0929a7073/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-5f7c83e0929a7073/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 16946.68it/s]\n",
      "02/12/2024 21:24:49 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:24:49 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2589.88it/s]\n",
      "02/12/2024 21:24:49 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:24:49 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:24:49 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-5f7c83e0929a7073/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2011.66it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:24:49,237 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:24:49,238 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:24:49,246 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:24:49,246 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:24:49,246 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:24:49,246 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:24:49,246 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:24:49,246 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:24:49,283 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:24:49,354 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:24:50,676 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:24:50,676 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:24:50,677 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:24:50,677 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/105 [00:00<?, ? examples/s]02/12/2024 21:24:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-5f7c83e0929a7073/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-2eff7cf27bd9df80.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/105 [00:00<?, ? examples/s]02/12/2024 21:24:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-5f7c83e0929a7073/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-ef4ddec09c623839.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/105 [00:00<?, ? examples/s]02/12/2024 21:24:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-5f7c83e0929a7073/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-8dd87c69c5b4da02.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/105 [00:00<?, ? examples/s]02/12/2024 21:24:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-5f7c83e0929a7073/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-48bef101d44ec440.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:24:52,005 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:24:52,005 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:24:52,005 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:24:52,005 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:24:52,005 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:24:52,005 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:24:52,005 >>   Total optimization steps = 8\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:24:52,006 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.64it/s][INFO|trainer.py:1912] 2024-02-12 21:24:57,641 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5.6369, 'train_samples_per_second': 1.419, 'train_steps_per_second': 1.419, 'train_loss': 0.4989193081855774, 'epoch': 8.0}\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.42it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:24:57,649 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:24:57,650 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:24:57,651 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:24:58,404 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:24:58,408 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:24:58,408 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.4989\n",
      "  train_runtime            = 0:00:05.63\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      1.419\n",
      "  train_steps_per_second   =      1.419\n",
      "02/12/2024 21:24:58 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:24:58,490 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:24:58,491 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:24:58,491 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.11it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =      0.868\n",
      "  eval_loss               =     0.3969\n",
      "  eval_runtime            = 0:00:00.21\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =      4.665\n",
      "  eval_steps_per_second   =      4.665\n",
      "  perplexity              =     1.4872\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:24:58,935 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8680351906158358}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ed3487a0f1465cb0579a45db4e1949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c31c357f244abb850558c9be6eb387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.7017171770334243], [0, 1, 7.65284811258316], [1, 0, 1.7324846714735032], [1, 1, 5.543031466007233], [2, 0, 1.7540715426206588], [2, 1, 5.026992207765579], [3, 0, 1.780388554930687], [3, 1, 4.529018920660019], [4, 0, 1.880611053109169], [4, 1, 4.2055415451526645]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:25:10 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:25:10 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-25-10_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:25:11 - INFO - datasets.builder - Using custom data configuration default-30b9e2f3ae0cdeb7\n",
      "02/12/2024 21:25:11 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:25:11 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-30b9e2f3ae0cdeb7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-30b9e2f3ae0cdeb7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 18517.90it/s]\n",
      "02/12/2024 21:25:11 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:25:11 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2824.45it/s]\n",
      "02/12/2024 21:25:11 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:25:11 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:25:11 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-30b9e2f3ae0cdeb7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2151.48it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:25:11,206 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:25:11,206 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:25:11,215 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:25:11,215 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:25:11,215 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:25:11,215 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:25:11,215 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:25:11,215 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:25:11,248 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:25:11,315 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:25:12,650 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:25:12,650 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:25:12,651 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:25:12,651 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:25:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-30b9e2f3ae0cdeb7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-13ff9adce98421db.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:25:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-30b9e2f3ae0cdeb7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-42f74471d7d65e2b.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:25:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-30b9e2f3ae0cdeb7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-61a502586a03f5e2.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:25:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-30b9e2f3ae0cdeb7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-9d69b562fe13c841.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:25:14,021 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:25:14,021 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:25:14,021 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:25:14,021 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:25:14,021 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:25:14,021 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:25:14,021 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:25:14,021 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.36it/s][INFO|trainer.py:1912] 2024-02-12 21:25:17,252 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3.2341, 'train_samples_per_second': 1.237, 'train_steps_per_second': 1.237, 'train_loss': 0.33090412616729736, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:25:17,258 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:25:17,259 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:25:17,260 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:25:17,820 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:25:17,821 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:25:17,821 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.3309\n",
      "  train_runtime            = 0:00:03.23\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.237\n",
      "  train_steps_per_second   =      1.237\n",
      "02/12/2024 21:25:17 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:25:17,850 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:25:17,850 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:25:17,850 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 13.23it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.9101\n",
      "  eval_loss               =      0.279\n",
      "  eval_runtime            = 0:00:00.30\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.488\n",
      "  eval_steps_per_second   =      6.488\n",
      "  perplexity              =     1.3218\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:25:18,475 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9100684261974584}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: championship, TO: pioneer, PATH: championship WEST pioneer', 'FROM: pin, TO: pioneer, PATH: pin NORTH pioneer', 'FROM: pin, TO: pilaf, PATH: pin SOUTH jet EAST pilaf', 'FROM: extent, TO: pioneer, PATH: extent EAST boom EAST pioneer', 'FROM: championship, TO: pioneer, PATH: championship NORTH championship WEST pioneer']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:26:01 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:26:01 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-26-00_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:26:01 - INFO - datasets.builder - Using custom data configuration default-e8e29ecadf9e1af7\n",
      "02/12/2024 21:26:01 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:26:01 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-e8e29ecadf9e1af7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-e8e29ecadf9e1af7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 17225.07it/s]\n",
      "02/12/2024 21:26:01 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:26:01 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1050.02it/s]\n",
      "02/12/2024 21:26:01 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:26:01 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:26:01 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-e8e29ecadf9e1af7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1945.41it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:26:01,590 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:26:01,591 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:26:01,599 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:26:01,599 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:26:01,599 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:26:01,599 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:26:01,599 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:26:01,599 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:26:01,636 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:26:01,705 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:26:03,020 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:26:03,020 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:26:03,021 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:26:03,021 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/106 [00:00<?, ? examples/s]02/12/2024 21:26:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e8e29ecadf9e1af7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-e40a7fb253f300ef.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/106 [00:00<?, ? examples/s]02/12/2024 21:26:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e8e29ecadf9e1af7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-e39909ef09026e53.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/106 [00:00<?, ? examples/s]02/12/2024 21:26:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e8e29ecadf9e1af7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-be55b1202106cf72.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/106 [00:00<?, ? examples/s]02/12/2024 21:26:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-e8e29ecadf9e1af7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-1236d09c50e494c4.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:26:04,321 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:26:04,321 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:26:04,321 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:26:04,321 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:26:04,321 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:26:04,321 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:26:04,321 >>   Total optimization steps = 8\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:26:04,321 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.62it/s][INFO|trainer.py:1912] 2024-02-12 21:26:10,027 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5.7066, 'train_samples_per_second': 1.402, 'train_steps_per_second': 1.402, 'train_loss': 0.46743664145469666, 'epoch': 8.0}\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.40it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:26:10,035 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:26:10,036 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:26:10,037 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:26:10,635 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:26:10,636 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:26:10,636 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.4674\n",
      "  train_runtime            = 0:00:05.70\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      1.402\n",
      "  train_steps_per_second   =      1.402\n",
      "02/12/2024 21:26:10 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:26:10,663 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:26:10,663 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:26:10,663 >>   Batch size = 1\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 118.94it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.8759\n",
      "  eval_loss               =      0.379\n",
      "  eval_runtime            = 0:00:00.16\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =       6.22\n",
      "  eval_steps_per_second   =       6.22\n",
      "  perplexity              =     1.4608\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:26:10,977 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8758553274682307}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6080dc51f2664a8398f9c23c6b0d873a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4149e466aa8d480aa71e6c380aa5387e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.7017171770334243], [0, 1, 7.65284811258316], [1, 0, 1.7324846714735032], [1, 1, 5.543031466007233], [2, 0, 1.7540715426206588], [2, 1, 5.026992207765579], [3, 0, 1.780388554930687], [3, 1, 4.529018920660019], [4, 0, 1.880611053109169], [4, 1, 4.2055415451526645], [5, 0, 1.840743950009346], [5, 1, 3.8170032680034636]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:26:22 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:26:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-26-22_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:26:23 - INFO - datasets.builder - Using custom data configuration default-f5af6e2b3ebf70a2\n",
      "02/12/2024 21:26:23 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:26:23 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-f5af6e2b3ebf70a2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-f5af6e2b3ebf70a2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 20213.51it/s]\n",
      "02/12/2024 21:26:23 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:26:23 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 3316.97it/s]\n",
      "02/12/2024 21:26:23 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:26:23 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:26:23 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-f5af6e2b3ebf70a2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2451.38it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:26:23,067 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:26:23,067 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:26:23,075 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:26:23,075 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:26:23,075 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:26:23,075 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:26:23,075 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:26:23,075 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:26:23,115 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:26:23,186 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:26:24,509 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:26:24,510 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:26:24,510 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:26:24,510 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:26:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-f5af6e2b3ebf70a2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-4fc991e720679790.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:26:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-f5af6e2b3ebf70a2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-0b1b82e9fb33a40b.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:26:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-f5af6e2b3ebf70a2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-9fa65b4e3ffc013f.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:26:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-f5af6e2b3ebf70a2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-62b01ab33f364155.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:26:25,704 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:26:25,704 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:26:25,704 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:26:25,704 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:26:25,704 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:26:25,704 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:26:25,704 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:26:25,704 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:02<00:00,  1.49it/s][INFO|trainer.py:1912] 2024-02-12 21:26:28,667 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2.9643, 'train_samples_per_second': 1.349, 'train_steps_per_second': 1.349, 'train_loss': 0.29767394065856934, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:02<00:00,  1.35it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:26:28,673 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:26:28,674 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:26:28,674 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:26:29,221 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:26:29,222 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:26:29,222 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.2977\n",
      "  train_runtime            = 0:00:02.96\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.349\n",
      "  train_steps_per_second   =      1.349\n",
      "02/12/2024 21:26:29 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:26:29,255 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:26:29,255 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:26:29,255 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 12.74it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.9115\n",
      "  eval_loss               =     0.2682\n",
      "  eval_runtime            = 0:00:00.31\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.378\n",
      "  eval_steps_per_second   =      6.378\n",
      "  perplexity              =     1.3076\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:26:29,786 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9115347018572825}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: championship, TO: pioneer, PATH: championship SOUTH signature WEST pioneer', 'FROM: pin, TO: pioneer, PATH: pin NORTH pioneer', 'FROM: pin, TO: pioneer, PATH: pin NORTH pioneer', 'FROM: pioneer, TO: boom, PATH: pioneer NORTH pin NORTH pioneer EAST signature EAST boom', 'FROM: championship, TO: pioneer, PATH: championship WEST boom WEST pioneer']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:27:12 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:27:12 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-27-12_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:27:13 - INFO - datasets.builder - Using custom data configuration default-89d16fbb4b0abca7\n",
      "02/12/2024 21:27:13 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:27:13 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-89d16fbb4b0abca7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-89d16fbb4b0abca7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 17549.39it/s]\n",
      "02/12/2024 21:27:13 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:27:13 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2624.72it/s]\n",
      "02/12/2024 21:27:13 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:27:13 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:27:13 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-89d16fbb4b0abca7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2145.97it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:27:13,336 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:27:13,337 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:27:13,344 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:27:13,344 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:27:13,344 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:27:13,344 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:27:13,344 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:27:13,344 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:27:13,378 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:27:13,448 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:27:14,877 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:27:14,877 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:27:14,878 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:27:14,878 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/108 [00:00<?, ? examples/s]02/12/2024 21:27:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-89d16fbb4b0abca7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-36b761ddea0971fa.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/108 [00:00<?, ? examples/s]02/12/2024 21:27:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-89d16fbb4b0abca7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-867c4f9a35b57981.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/108 [00:00<?, ? examples/s]02/12/2024 21:27:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-89d16fbb4b0abca7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-b67cc6330f47dbd4.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/108 [00:00<?, ? examples/s]02/12/2024 21:27:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-89d16fbb4b0abca7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-ea6438c935aea05d.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:27:16,170 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:27:16,171 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:27:16,171 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:27:16,171 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:27:16,171 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:27:16,171 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:27:16,171 >>   Total optimization steps = 8\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:27:16,171 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.63it/s][INFO|trainer.py:1912] 2024-02-12 21:27:21,895 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5.7257, 'train_samples_per_second': 1.397, 'train_steps_per_second': 1.397, 'train_loss': 0.47630029916763306, 'epoch': 8.0}\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.40it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:27:21,905 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:27:21,906 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:27:21,907 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:27:22,544 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:27:22,545 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:27:22,545 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.4763\n",
      "  train_runtime            = 0:00:05.72\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      1.397\n",
      "  train_steps_per_second   =      1.397\n",
      "02/12/2024 21:27:22 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:27:22,581 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:27:22,581 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:27:22,581 >>   Batch size = 1\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 103.07it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.8661\n",
      "  eval_loss               =     0.3828\n",
      "  eval_runtime            = 0:00:00.17\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =      5.813\n",
      "  eval_steps_per_second   =      5.813\n",
      "  perplexity              =     1.4664\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:27:22,932 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8660801564027371}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7cbd966ef944a7b32e2c74a7ffe16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19db145b68504ef0992e89dea9139afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.7017171770334243], [0, 1, 7.65284811258316], [1, 0, 1.7324846714735032], [1, 1, 5.543031466007233], [2, 0, 1.7540715426206588], [2, 1, 5.026992207765579], [3, 0, 1.780388554930687], [3, 1, 4.529018920660019], [4, 0, 1.880611053109169], [4, 1, 4.2055415451526645], [5, 0, 1.840743950009346], [5, 1, 3.8170032680034636], [6, 0, 1.9109028697013855], [6, 1, 3.6854703187942506]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:27:34 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:27:34 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-27-34_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:27:35 - INFO - datasets.builder - Using custom data configuration default-c576311f78a431c1\n",
      "02/12/2024 21:27:35 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:27:35 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-c576311f78a431c1/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-c576311f78a431c1/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 22982.49it/s]\n",
      "02/12/2024 21:27:35 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:27:35 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 3425.32it/s]\n",
      "02/12/2024 21:27:35 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:27:35 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:27:35 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-c576311f78a431c1/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2538.16it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:27:35,108 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:27:35,108 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:27:35,116 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:27:35,116 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:27:35,116 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:27:35,116 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:27:35,116 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:27:35,116 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:27:35,154 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:27:35,227 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:27:36,555 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:27:36,555 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:27:36,556 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:27:36,556 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:27:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-c576311f78a431c1/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-cffdf6eb7b17cd26.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:27:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-c576311f78a431c1/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-73875b79a0151a72.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:27:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-c576311f78a431c1/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-8d4d95a37ee68dd9.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:27:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-c576311f78a431c1/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-158bd03b5cbe18b5.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:27:37,870 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:27:37,870 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:27:37,870 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:27:37,870 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:27:37,870 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:27:37,870 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:27:37,870 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:27:37,870 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.40it/s][INFO|trainer.py:1912] 2024-02-12 21:27:41,104 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3.235, 'train_samples_per_second': 1.236, 'train_steps_per_second': 1.236, 'train_loss': 0.3035561144351959, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:27:41,107 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:27:41,108 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:27:41,108 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:27:41,686 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:27:41,689 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:27:41,689 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.3036\n",
      "  train_runtime            = 0:00:03.23\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.236\n",
      "  train_steps_per_second   =      1.236\n",
      "02/12/2024 21:27:41 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:27:41,729 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:27:41,729 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:27:41,729 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 11.84it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.9169\n",
      "  eval_loss               =     0.2573\n",
      "  eval_runtime            = 0:00:00.32\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.205\n",
      "  eval_steps_per_second   =      6.205\n",
      "  perplexity              =     1.2935\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:27:42,187 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.916911045943304}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: pin, TO: pioneer, PATH: pin EAST pioneer', 'FROM: signature, TO: pioneer, PATH: signature WEST pioneer', 'FROM: pin, TO: pilaf, PATH: pin SOUTH pioneer SOUTH pilaf', 'FROM: pilaf, TO: pioneer, PATH: pilaf NORTH pioneer', 'FROM: pioneer, TO: boom, PATH: pioneer SOUTH pilaf WEST boom']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:28:21 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:28:21 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-28-21_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:28:22 - INFO - datasets.builder - Using custom data configuration default-705d14fe1f21e174\n",
      "02/12/2024 21:28:22 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:28:22 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-705d14fe1f21e174/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-705d14fe1f21e174/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 16384.00it/s]\n",
      "02/12/2024 21:28:22 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:28:22 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1131.15it/s]\n",
      "02/12/2024 21:28:22 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:28:22 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:28:22 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-705d14fe1f21e174/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2118.34it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:28:22,632 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:28:22,632 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:28:22,641 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:28:22,641 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:28:22,641 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:28:22,641 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:28:22,641 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:28:22,641 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:28:22,677 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:28:22,745 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:28:24,081 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:28:24,082 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:28:24,082 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:28:24,082 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/102 [00:00<?, ? examples/s]02/12/2024 21:28:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-705d14fe1f21e174/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-0704ba316574f0e7.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/102 [00:00<?, ? examples/s]02/12/2024 21:28:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-705d14fe1f21e174/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a3d62d42f617e6d5.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/102 [00:00<?, ? examples/s]02/12/2024 21:28:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-705d14fe1f21e174/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-30334c4cc84eea76.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/102 [00:00<?, ? examples/s]02/12/2024 21:28:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-705d14fe1f21e174/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-0af8734510469232.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:28:25,630 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:28:25,630 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:28:25,630 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:28:25,630 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:28:25,630 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:28:25,630 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:28:25,630 >>   Total optimization steps = 8\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:28:25,630 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.63it/s][INFO|trainer.py:1912] 2024-02-12 21:28:31,323 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5.6957, 'train_samples_per_second': 1.405, 'train_steps_per_second': 1.405, 'train_loss': 0.47142091393470764, 'epoch': 8.0}\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.40it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:28:31,332 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:28:31,333 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:28:31,334 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:28:31,898 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:28:31,899 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:28:31,899 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.4714\n",
      "  train_runtime            = 0:00:05.69\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      1.405\n",
      "  train_steps_per_second   =      1.405\n",
      "02/12/2024 21:28:31 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:28:31,931 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:28:31,931 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:28:31,931 >>   Batch size = 1\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 105.31it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.8778\n",
      "  eval_loss               =     0.3782\n",
      "  eval_runtime            = 0:00:00.16\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =       5.98\n",
      "  eval_steps_per_second   =       5.98\n",
      "  perplexity              =     1.4596\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:28:32,325 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8778103616813294}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e26d2f712cc4a5a8e662bce1bfecaed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7e8d7f49e145a0b35dccde2d6865fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.7017171770334243], [0, 1, 7.65284811258316], [1, 0, 1.7324846714735032], [1, 1, 5.543031466007233], [2, 0, 1.7540715426206588], [2, 1, 5.026992207765579], [3, 0, 1.780388554930687], [3, 1, 4.529018920660019], [4, 0, 1.880611053109169], [4, 1, 4.2055415451526645], [5, 0, 1.840743950009346], [5, 1, 3.8170032680034636], [6, 0, 1.9109028697013855], [6, 1, 3.6854703187942506], [7, 0, 1.9504795491695404], [7, 1, 3.3227295219898223]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:28:43 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:28:43 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-28-43_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:28:44 - INFO - datasets.builder - Using custom data configuration default-52da0c9731f96f70\n",
      "02/12/2024 21:28:44 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:28:44 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-52da0c9731f96f70/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-52da0c9731f96f70/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 16131.94it/s]\n",
      "02/12/2024 21:28:44 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:28:44 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2631.31it/s]\n",
      "02/12/2024 21:28:44 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:28:44 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:28:44 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-52da0c9731f96f70/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2141.59it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:28:44,575 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:28:44,576 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:28:44,584 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:28:44,584 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:28:44,584 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:28:44,584 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:28:44,584 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:28:44,584 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:28:44,618 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:28:44,686 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:28:46,008 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:28:46,008 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:28:46,009 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:28:46,009 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:28:46 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-52da0c9731f96f70/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-3519d7ba2a2f029f.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:28:46 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-52da0c9731f96f70/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-94bb037f013073bf.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:28:46 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-52da0c9731f96f70/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-b387b7dc5e092d03.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:28:46 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-52da0c9731f96f70/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-ec4279c70d7b85e0.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:28:47,265 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:28:47,265 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:28:47,265 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:28:47,265 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:28:47,265 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:28:47,265 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:28:47,265 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:28:47,265 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.42it/s][INFO|trainer.py:1912] 2024-02-12 21:28:50,392 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3.1277, 'train_samples_per_second': 1.279, 'train_steps_per_second': 1.279, 'train_loss': 0.28187233209609985, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.28it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:28:50,396 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:28:50,398 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:28:50,398 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:28:50,962 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:28:50,963 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:28:50,963 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.2819\n",
      "  train_runtime            = 0:00:03.12\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.279\n",
      "  train_steps_per_second   =      1.279\n",
      "02/12/2024 21:28:50 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:28:50,994 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:28:50,994 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:28:50,994 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 13.06it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.9203\n",
      "  eval_loss               =     0.2449\n",
      "  eval_runtime            = 0:00:00.31\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      6.449\n",
      "  eval_steps_per_second   =      6.449\n",
      "  perplexity              =     1.2775\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:28:51,591 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9203323558162267}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: championship, TO: pioneer, PATH: championship SOUTH signature WEST pioneer', 'FROM: pilaf, TO: pioneer, PATH: pilaf NORTH pioneer', 'FROM: pin, TO: signature, PATH: pin EAST pioneer NORTH pilaf EAST pioneer EAST signature', 'FROM: championship, TO: championship, PATH: championship WEST boom SOUTH pioneer SOUTH pilaf WEST championship', 'FROM: championship']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:29:31 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:29:31 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-29-30_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:29:31 - INFO - datasets.builder - Using custom data configuration default-1c21b706235a6297\n",
      "02/12/2024 21:29:31 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:29:31 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-1c21b706235a6297/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-1c21b706235a6297/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 18517.90it/s]\n",
      "02/12/2024 21:29:31 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:29:31 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2801.81it/s]\n",
      "02/12/2024 21:29:31 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:29:31 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:29:31 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-1c21b706235a6297/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2141.59it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:29:31,541 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:29:31,541 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:29:31,549 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:29:31,549 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:29:31,549 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:29:31,549 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:29:31,549 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:29:31,549 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:29:31,583 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:29:31,650 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:29:32,975 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:29:32,975 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:29:32,976 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:29:32,976 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/103 [00:00<?, ? examples/s]02/12/2024 21:29:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1c21b706235a6297/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-42bf40d6967e1e5c.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/103 [00:00<?, ? examples/s]02/12/2024 21:29:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1c21b706235a6297/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-b5687d571cf8282c.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/103 [00:00<?, ? examples/s]02/12/2024 21:29:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1c21b706235a6297/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-503d48d571ceb93c.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/103 [00:00<?, ? examples/s]02/12/2024 21:29:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-1c21b706235a6297/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-2125936958b08fdf.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:29:34,281 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:29:34,281 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:29:34,281 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:29:34,281 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:29:34,281 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:29:34,281 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:29:34,281 >>   Total optimization steps = 8\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:29:34,282 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.63it/s][INFO|trainer.py:1912] 2024-02-12 21:29:39,974 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5.6948, 'train_samples_per_second': 1.405, 'train_steps_per_second': 1.405, 'train_loss': 0.478937566280365, 'epoch': 8.0}\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.40it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:29:39,982 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:29:39,983 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:29:39,983 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:29:40,586 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:29:40,587 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:29:40,587 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.4789\n",
      "  train_runtime            = 0:00:05.69\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      1.405\n",
      "  train_steps_per_second   =      1.405\n",
      "02/12/2024 21:29:40 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:29:40,618 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:29:40,618 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:29:40,618 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 55.06it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.8612\n",
      "  eval_loss               =     0.3962\n",
      "  eval_runtime            = 0:00:00.28\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =      3.538\n",
      "  eval_steps_per_second   =      3.538\n",
      "  perplexity              =     1.4862\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:29:41,137 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8611925708699902}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d862ada3c841efa134677bf2794946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6cb43b3e6e4d7e9ad03f3983749ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.7017171770334243], [0, 1, 7.65284811258316], [1, 0, 1.7324846714735032], [1, 1, 5.543031466007233], [2, 0, 1.7540715426206588], [2, 1, 5.026992207765579], [3, 0, 1.780388554930687], [3, 1, 4.529018920660019], [4, 0, 1.880611053109169], [4, 1, 4.2055415451526645], [5, 0, 1.840743950009346], [5, 1, 3.8170032680034636], [6, 0, 1.9109028697013855], [6, 1, 3.6854703187942506], [7, 0, 1.9504795491695404], [7, 1, 3.3227295219898223], [8, 0, 2.081262707710266], [8, 1, 3.1387059092521667]]\n",
      "NREM phase\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:29:53 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:29:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-29-52_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:29:54 - INFO - datasets.builder - Using custom data configuration default-3f934a099b3600ae\n",
      "02/12/2024 21:29:54 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:29:54 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-3f934a099b3600ae/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-3f934a099b3600ae/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 16416.06it/s]\n",
      "02/12/2024 21:29:54 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:29:54 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2430.07it/s]\n",
      "02/12/2024 21:29:54 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:29:54 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:29:54 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-3f934a099b3600ae/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1890.18it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:29:54,376 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:29:54,377 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:29:54,386 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:29:54,386 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:29:54,386 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:29:54,386 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:29:54,386 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:29:54,386 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:29:54,425 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:29:54,499 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:29:55,838 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:29:55,838 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:29:55,839 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:29:55,839 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:29:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-3f934a099b3600ae/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-13f1b7257cefc437.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/100 [00:00<?, ? examples/s]02/12/2024 21:29:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-3f934a099b3600ae/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-8dec42e7e6c0f7dc.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:29:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-3f934a099b3600ae/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a304f3415fa8be13.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/100 [00:00<?, ? examples/s]02/12/2024 21:29:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-3f934a099b3600ae/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-560b62341f18a107.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:29:57,009 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:29:57,009 >>   Num examples = 2\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:29:57,009 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:29:57,009 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:29:57,009 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:29:57,009 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:29:57,009 >>   Total optimization steps = 4\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:29:57,009 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.33it/s][INFO|trainer.py:1912] 2024-02-12 21:30:00,364 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3.3563, 'train_samples_per_second': 1.192, 'train_steps_per_second': 1.192, 'train_loss': 0.275614857673645, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.19it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:30:00,369 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:30:00,370 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:30:00,370 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:30:00,982 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:30:00,983 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:30:00,983 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.2756\n",
      "  train_runtime            = 0:00:03.35\n",
      "  train_samples            =          2\n",
      "  train_samples_per_second =      1.192\n",
      "  train_steps_per_second   =      1.192\n",
      "02/12/2024 21:30:01 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:30:01,016 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:30:01,016 >>   Num examples = 2\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:30:01,016 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  8.75it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_accuracy           =     0.9106\n",
      "  eval_loss               =     0.2454\n",
      "  eval_runtime            = 0:00:00.38\n",
      "  eval_samples            =          2\n",
      "  eval_samples_per_second =      5.145\n",
      "  eval_steps_per_second   =      5.145\n",
      "  perplexity              =     1.2781\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:30:01,583 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9105571847507331}]}\n",
      "REM phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM: invader, TO: pioneer, PATH: invader WEST pioneer', 'FROM: championship, TO: pioneer, PATH: championship SOUTH signature WEST championship WEST boom SOUTH pioneer', 'FROM: championship, TO: pioneer, PATH: championship WEST boom', 'FROM: pilaf, TO: signature, PATH: pilaf EAST pioneer EAST signature', 'FROM: boom, TO: signature, PATH: boom EAST pioneer NORTH championship EAST signature']\n",
      "About to train model from spatial_model_1 and save to spatial_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "02/12/2024 21:30:37 - WARNING - __main__ - Process rank: 0, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/12/2024 21:30:37 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=spatial_model_1/runs/Feb12_21-30-37_Eleanors-MBP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=8.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=spatial_model_1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=spatial_model_1,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/12/2024 21:30:37 - INFO - datasets.builder - Using custom data configuration default-951b73b10971580e\n",
      "02/12/2024 21:30:37 - INFO - datasets.info - Loading Dataset Infos from /Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "02/12/2024 21:30:37 - INFO - datasets.builder - Generating dataset text (/Users/eleanorspens/.cache/huggingface/datasets/text/default-951b73b10971580e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /Users/eleanorspens/.cache/huggingface/datasets/text/default-951b73b10971580e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 15857.48it/s]\n",
      "02/12/2024 21:30:37 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/12/2024 21:30:37 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1069.43it/s]\n",
      "02/12/2024 21:30:37 - INFO - datasets.builder - Generating train split\n",
      "02/12/2024 21:30:37 - INFO - datasets.builder - Generating validation split\n",
      "02/12/2024 21:30:37 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /Users/eleanorspens/.cache/huggingface/datasets/text/default-951b73b10971580e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2197.70it/s]\n",
      "[INFO|configuration_utils.py:715] 2024-02-12 21:30:37,839 >> loading configuration file spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-02-12 21:30:37,839 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"spatial_model_1\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:30:37,848 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:30:37,848 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:30:37,848 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:30:37,848 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:30:37,848 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-02-12 21:30:37,848 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3083] 2024-02-12 21:30:37,884 >> loading weights file spatial_model_1/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:30:37,952 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3874] 2024-02-12 21:30:39,260 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3882] 2024-02-12 21:30:39,260 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at spatial_model_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-02-12 21:30:39,261 >> loading configuration file spatial_model_1/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-12 21:30:39,261 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|              | 0/101 [00:00<?, ? examples/s]02/12/2024 21:30:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-951b73b10971580e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-686b2406d4e2e5fa.arrow\n",
      "Running tokenizer on dataset:   0%|              | 0/101 [00:00<?, ? examples/s]02/12/2024 21:30:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-951b73b10971580e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-9505e301cbaa4faf.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/101 [00:00<?, ? examples/s]02/12/2024 21:30:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-951b73b10971580e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-18bc5b0b6ba33729.arrow\n",
      "Grouping texts in chunks of 1024:   0%|          | 0/101 [00:00<?, ? examples/s]02/12/2024 21:30:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /Users/eleanorspens/.cache/huggingface/datasets/text/default-951b73b10971580e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-1ac8696821304324.arrow\n",
      "[INFO|trainer.py:1674] 2024-02-12 21:30:40,645 >> ***** Running training *****  \n",
      "[INFO|trainer.py:1675] 2024-02-12 21:30:40,645 >>   Num examples = 1\n",
      "[INFO|trainer.py:1676] 2024-02-12 21:30:40,645 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:1677] 2024-02-12 21:30:40,645 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1680] 2024-02-12 21:30:40,645 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1681] 2024-02-12 21:30:40,645 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1682] 2024-02-12 21:30:40,645 >>   Total optimization steps = 8\n",
      "[INFO|trainer.py:1683] 2024-02-12 21:30:40,646 >>   Number of trainable parameters = 124,439,808\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.69it/s][INFO|trainer.py:1912] 2024-02-12 21:30:46,108 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5.4638, 'train_samples_per_second': 1.464, 'train_steps_per_second': 1.464, 'train_loss': 0.5246599912643433, 'epoch': 8.0}\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.46it/s]\n",
      "[INFO|trainer.py:2816] 2024-02-12 21:30:46,113 >> Saving model checkpoint to spatial_model_1\n",
      "[INFO|configuration_utils.py:461] 2024-02-12 21:30:46,115 >> Configuration saved in spatial_model_1/config.json\n",
      "[INFO|configuration_utils.py:564] 2024-02-12 21:30:46,115 >> Configuration saved in spatial_model_1/generation_config.json\n",
      "[INFO|modeling_utils.py:2193] 2024-02-12 21:30:46,721 >> Model weights saved in spatial_model_1/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2428] 2024-02-12 21:30:46,726 >> tokenizer config file saved in spatial_model_1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2437] 2024-02-12 21:30:46,726 >> Special tokens file saved in spatial_model_1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_loss               =     0.5247\n",
      "  train_runtime            = 0:00:05.46\n",
      "  train_samples            =          1\n",
      "  train_samples_per_second =      1.464\n",
      "  train_steps_per_second   =      1.464\n",
      "02/12/2024 21:30:46 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3093] 2024-02-12 21:30:46,765 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3095] 2024-02-12 21:30:46,765 >>   Num examples = 1\n",
      "[INFO|trainer.py:3098] 2024-02-12 21:30:46,765 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.62it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.8534\n",
      "  eval_loss               =     0.4357\n",
      "  eval_runtime            = 0:00:00.18\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =       5.49\n",
      "  eval_steps_per_second   =       5.49\n",
      "  perplexity              =     1.5461\n",
      "[INFO|modelcard.py:452] 2024-02-12 21:30:47,367 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8533724340175953}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9421876fce45178a10d6b2d93ed27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe3de94752d4714aeb947cda86540f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results so far:\n",
      "[[0, 0, 1.7017171770334243], [0, 1, 7.65284811258316], [1, 0, 1.7324846714735032], [1, 1, 5.543031466007233], [2, 0, 1.7540715426206588], [2, 1, 5.026992207765579], [3, 0, 1.780388554930687], [3, 1, 4.529018920660019], [4, 0, 1.880611053109169], [4, 1, 4.2055415451526645], [5, 0, 1.840743950009346], [5, 1, 3.8170032680034636], [6, 0, 1.9109028697013855], [6, 1, 3.6854703187942506], [7, 0, 1.9504795491695404], [7, 1, 3.3227295219898223], [8, 0, 2.081262707710266], [8, 1, 3.1387059092521667], [9, 0, 2.084974059462547], [9, 1, 2.9781828105449675]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eleanorspens/Documents/PhD Code/algonauts/algovenv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: boom, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: championship, Predicted location: championship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: apartment, Predicted location: jet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: championship, Predicted location: championship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: boom, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: extent, Predicted location: extent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: boom, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: apartment, Predicted location: apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: championship, Predicted location: championship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: ovary, Predicted location: jet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: extent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: forum, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: grasshopper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: dictaphone, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: jet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: jet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: extent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: forum, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: extent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: pilaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: dictaphone, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: jet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grasshopper, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: ovary, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: extent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: pin\n",
      "Correct location: metabolite, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: boom, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: championship, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: apartment, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: championship, Predicted location: championship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: boom, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: extent, Predicted location: championship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: boom, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pin, Predicted location: pin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: apartment, Predicted location: championship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: signature, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pilaf, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: championship, Predicted location: championship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: pioneer, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: championship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: nucleotidase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: ovary, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: nucleotidase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: backburn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: championship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: grasshopper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: nucleotidase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: forum, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: grasshopper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: grape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: dictaphone, Predicted location: grasshopper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: ovary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: metabolite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: backburn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: signature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: nucleotidase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: forum, Predicted location: forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: grasshopper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: dictaphone, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: metabolite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grasshopper, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: technician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: pioneer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: ovary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: backburn, Predicted location: forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: nucleotidase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: ovary, Predicted location: forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: nucleotidase, Predicted location: forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: technician, Predicted location: metabolite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: metabolite, Predicted location: boom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: metabolite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct location: grape, Predicted location: pioneer\n",
      "Correct location: metabolite, Predicted location: pioneer\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[([[0, 0, 0.75], [0, 1, 0.0], [1, 0, 0.55], [1, 1, 0.1]],\n",
       "  [[0, 0, 1.7017171770334243],\n",
       "   [0, 1, 7.65284811258316],\n",
       "   [1, 0, 1.7324846714735032],\n",
       "   [1, 1, 5.543031466007233],\n",
       "   [2, 0, 1.7540715426206588],\n",
       "   [2, 1, 5.026992207765579],\n",
       "   [3, 0, 1.780388554930687],\n",
       "   [3, 1, 4.529018920660019],\n",
       "   [4, 0, 1.880611053109169],\n",
       "   [4, 1, 4.2055415451526645],\n",
       "   [5, 0, 1.840743950009346],\n",
       "   [5, 1, 3.8170032680034636],\n",
       "   [6, 0, 1.9109028697013855],\n",
       "   [6, 1, 3.6854703187942506],\n",
       "   [7, 0, 1.9504795491695404],\n",
       "   [7, 1, 3.3227295219898223],\n",
       "   [8, 0, 2.081262707710266],\n",
       "   [8, 1, 3.1387059092521667],\n",
       "   [9, 0, 2.084974059462547],\n",
       "   [9, 1, 2.9781828105449675]])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_schedule_multiple_seeds(seeds=[0,1,2], \n",
    "                                   total_eps=100, \n",
    "                                   num_cycles=10, \n",
    "                                   start_fraction_rem=0.8, \n",
    "                                   end_fraction_rem=0.8,\n",
    "                                   num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6fd1fa-ee9f-4c41-a0b2-d3170e7bff0e",
   "metadata": {},
   "source": [
    "#### Number of cycles comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d782065-6433-4f60-af56-5f879c766dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_schedule_multiple_seeds(seeds=[0,1,2], \n",
    "                                   total_eps=100, \n",
    "                                   num_cycles=5, \n",
    "                                   start_fraction_rem=0.5, \n",
    "                                   end_fraction_rem=0.5,\n",
    "                                   num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9b0b0-9116-4774-ad5d-30f1f6617be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_with_schedule_multiple_seeds(seeds=[0,1,2], \n",
    "                                   total_eps=100, \n",
    "                                   num_cycles=10, \n",
    "                                   start_fraction_rem=0.5, \n",
    "                                   end_fraction_rem=0.5,\n",
    "                                   num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba124bb-71a0-4b2a-a9f7-738966d72d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_schedule_multiple_seeds(seeds=[0,1,2], \n",
    "                                   total_eps=100, \n",
    "                                   num_cycles=25, \n",
    "                                   start_fraction_rem=0.5, \n",
    "                                   end_fraction_rem=0.5,\n",
    "                                   num=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
