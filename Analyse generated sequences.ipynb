{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b0aaad-985d-44b4-915a-a78008d23858",
   "metadata": {},
   "source": [
    "#### Analyse generated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c819ce0c-3092-42a8-b8fb-a984f8ec82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from testing_utils import *\n",
    "\n",
    "# Path to your pickle file\n",
    "input_file = 'word_freq_results_imagined_colab3.pkl'\n",
    "\n",
    "# Loading the data from the pickle file\n",
    "with open(input_file, 'rb') as file:\n",
    "    results_dict = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc9f8e-6697-4a70-b6dc-35ee77450885",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2c1e7-5eca-4a0d-b32f-109d4ae8d7a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(results_dict)['temp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b80210-3dad-497f-9cac-dbd1d25100c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_sequences(results):\n",
    "    # Extract necessary data\n",
    "    current_env = results['model']  # The current environment index\n",
    "    training_strs = results['training_strs']\n",
    "    testing_strs = results['testing_strs']\n",
    "    train_size = results['train_size']\n",
    "    seqs = results['seqs']\n",
    "\n",
    "    # Initialize results storage\n",
    "    analysis_results = {'real': [], 'valid': [], 'neither': []}\n",
    "\n",
    "    # Iterate through each sequence in seqs\n",
    "    for seq in seqs:\n",
    "        found_as_real = False\n",
    "        found_as_valid = False\n",
    "\n",
    "        # Check for real sequences in previous phases\n",
    "        for i in range(current_env):\n",
    "            if seq in training_strs[i][0:train_size]:\n",
    "                analysis_results['real'].append(seq)\n",
    "                found_as_real = True\n",
    "                break  # Stop searching if found\n",
    "\n",
    "        if not found_as_real:\n",
    "            # Check for valid sequences if not found as real\n",
    "            for i in range(current_env):\n",
    "                if seq in training_strs[i][train_size:] or seq in testing_strs[i]:\n",
    "                    analysis_results['valid'].append(seq)\n",
    "                    found_as_valid = True\n",
    "                    break  # Stop searching if found\n",
    "\n",
    "        if not (found_as_real or found_as_valid):\n",
    "            # If the sequence is neither real nor valid\n",
    "            analysis_results['neither'].append(seq)\n",
    "\n",
    "    return analysis_results\n",
    "\n",
    "# Call the function with your results_dict\n",
    "analysis_results = analyze_sequences(results_dict[24])\n",
    "analysis_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd8751-445e-47ea-893c-06fc6a1ffb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming results_dict now includes a way to differentiate between temperatures\n",
    "# For example, results_dict could be a list of dicts, each with a different temperature setting\n",
    "\n",
    "# Modify the analyze_sequences function if necessary to work with your data structure\n",
    "\n",
    "def analyze_sequences_by_temperature(results_dicts):\n",
    "    temperature_results = {}\n",
    "\n",
    "    for results in results_dicts:\n",
    "        temp = results['temp']  # Assuming 'temp' is how temperature is recorded\n",
    "        if temp == -1:  # Skip the analysis for temperature -1\n",
    "            continue\n",
    "        analysis_results = analyze_sequences(results)\n",
    "        temperature_results[temp] = {\n",
    "            'real': len(analysis_results['real']),\n",
    "            'valid': len(analysis_results['valid']),\n",
    "            'neither': len(analysis_results['neither'])\n",
    "        }\n",
    "\n",
    "    return temperature_results\n",
    "\n",
    "# Now, analyze your sequences\n",
    "temperature_results = analyze_sequences_by_temperature(results_dict)\n",
    "\n",
    "# Visualize the results\n",
    "def visualize_results(temperature_results):\n",
    "    temps = sorted(temperature_results.keys())\n",
    "    reals = [temperature_results[temp]['real'] for temp in temps]\n",
    "    valids = [temperature_results[temp]['valid'] for temp in temps]\n",
    "    neithers = [temperature_results[temp]['neither'] for temp in temps]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(temps, reals, label='Real', marker='o')\n",
    "    plt.plot(temps, valids, label='Valid', marker='s')\n",
    "    plt.plot(temps, neithers, label='Neither', marker='^')\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Effect of Temperature on Sequence Distribution')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Call visualize_results with your actual temperature_results\n",
    "visualize_results(temperature_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a189d303-8852-4190-8ab2-5e9406e80ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195bfd64-5e59-454c-bb82-7354ab19a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "class GPT:\n",
    "\n",
    "    def __init__(self, base_model):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(base_model)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(base_model)\n",
    "\n",
    "    def continue_input(self, input_sequence, max_length=200, num_return_sequences=1, no_repeat_ngram_size=0,\n",
    "                       do_sample=False, temperature=0.7, num_beams=1):\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(input_sequence, return_tensors='pt')\n",
    "\n",
    "        # Generate text\n",
    "        output = self.model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_beams,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            # penalty_alpha=0.6, \n",
    "            top_k=9\n",
    "        )\n",
    "\n",
    "        # Decode the output\n",
    "        sequence = output[0].tolist()\n",
    "        text = self.tokenizer.decode(sequence)\n",
    "        return text\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1ced4-0b69-4047-ae50-c502c7a5fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT('spatial_model_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12892c4c-8633-44fb-a606-523498977fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = results_dict[0]['training_strs'][0][0:50]\n",
    "test_seqs = results_dict[0]['training_strs'][0][50:]\n",
    "test_seqs.extend(results_dict[0]['testing_strs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f97d6-48d8-479b-b14e-491fbe821ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_data_subset(test_data, train_data):\n",
    "    train_starts = [train_seq.split('PATH:')[0] for train_seq in train_data]\n",
    "    # Filter test_data where the start is not in train_starts\n",
    "    subset = [test_seq for test_seq in test_data if test_seq.split('PATH:')[0] not in train_starts]\n",
    "    return subset\n",
    "\n",
    "for seq in test_data_subset(test_seqs, train_seqs):\n",
    "    print(seq)\n",
    "    seq = model.continue_input(seq[:seq.index('PATH:')+5], do_sample=False, num_beams=5)\n",
    "    seq = seq[:seq.index('\\n')]\n",
    "    print(seq)\n",
    "        \n",
    "    if seq in train_seqs:\n",
    "        print(\"real\")\n",
    "    elif seq in test_seqs:\n",
    "        print(\"valid\")\n",
    "    else:\n",
    "        print(\"invalid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8e073-3ddd-48ec-b1ed-ef0a5d5a4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    seqs = model.continue_input('FROM:', temperature=0.2, do_sample=True, max_length=200).split('\\n')\n",
    "    \n",
    "    for seq in seqs:\n",
    "        print(seq)\n",
    "        if seq in train_seqs:\n",
    "            print(\"real\")\n",
    "        elif seq in test_seqs:\n",
    "            print(\"valid\")\n",
    "        else:\n",
    "            print(\"invalid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253319e-85e4-472d-bd6b-709d29e022f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    seq = model.continue_input('FROM:', temperature=0.5, do_sample=True)\n",
    "\n",
    "    seqs = seq.split('\\n')\n",
    "    #seq = seq[:seq.index('\\n')]\n",
    "    for seq in seqs:\n",
    "        print(seq)\n",
    "\n",
    "        if seq in train_seqs:\n",
    "            print(\"real\")\n",
    "        elif seq in test_seqs:\n",
    "            print(\"valid\")\n",
    "        else:\n",
    "            print(\"invalid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96d699-120d-4f77-bcb4-df77cc33546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.continue_input('FROM:', temperature=0.8, do_sample=True).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b0c81-5243-4eb0-94ef-a0a69bd7d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.continue_input('FROM:', temperature=1.5, do_sample=True).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e58df-7991-49b6-96a5-ad9c25facda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b817e8-0c02-40d4-8c63-e1c7ff985500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf23dd-9cda-46bd-94da-d33cada19c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('replay_results_imagined_colab3.csv')\n",
    "df = df[df['test_type'] == 'next_node']\n",
    "\n",
    "# Group by 'Sample_Size', 'Trained_On', and 'Tested_On', and calculate mean and SEM\n",
    "grouped = df.groupby(['temp', 'trained_on', 'tested_on'])\n",
    "mean_df = grouped['accuracy'].mean().reset_index()\n",
    "sem_df = grouped['accuracy'].sem().reset_index()\n",
    "\n",
    "sem_df['accuracy'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e296f-2d22-4f98-a938-beddef056ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = mean_df['temp'].unique()\n",
    "num_env = df['trained_on'].nunique()\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(len(vals), 1, figsize=(8, 16), sharex=True)\n",
    "\n",
    "# Iterate over each sample size and create a subplot\n",
    "for i, val in enumerate(vals):\n",
    "    df_sample_mean = mean_df[mean_df['temp'] == val]\n",
    "    df_sample_sem = sem_df[sem_df['temp'] == val]\n",
    "\n",
    "    for tested_on in range(num_env):\n",
    "        # Filter the mean and SEM dataframes for the specific 'Tested_On' value\n",
    "        means = df_sample_mean[df_sample_mean['tested_on'] == tested_on]['accuracy']\n",
    "        sems = df_sample_sem[df_sample_sem['tested_on'] == tested_on]['accuracy']\n",
    "        trained_on_values = df_sample_mean[df_sample_mean['tested_on'] == tested_on]['trained_on']\n",
    "        \n",
    "        # Plot error bars\n",
    "        axes[i].errorbar(trained_on_values, means, yerr=sems, label=f'Tested on Env {tested_on}', marker='o')\n",
    "\n",
    "    letter = string.ascii_lowercase[i]\n",
    "    axes[i].set_title(f'{letter}) {val} self-generated samples')\n",
    "    axes[i].set_ylabel('Accuracy')\n",
    "    axes[i].set_ylim((0,1))\n",
    "    axes[i].legend()\n",
    "\n",
    "# Set common labels and title\n",
    "axes[-1].set_xlabel('Trained On Environment')\n",
    "axes[-1].set_xticks(range(num_env))\n",
    "# plt.suptitle('Mean Model Accuracy Across Trials with SEM')\n",
    "plt.savefig('Number of samples effect three trials.png', dpi=500)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2769126-ac32-44a8-b960-745a70f43436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
