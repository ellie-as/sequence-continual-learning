{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a231f9",
   "metadata": {},
   "source": [
    "### Continual learning and generative replay\n",
    "\n",
    "#### Installation:\n",
    "\n",
    "Local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.24.2\n",
    "!pip tensorflow-macos==2.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f726ba-f622-4c8b-b0af-9330f10a4277",
   "metadata": {},
   "source": [
    "Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba8f20-ee5d-44c4-8d59-358fb79192c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wonderwords evaluate datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12409883",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c95a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from continual_learning_utils import *\n",
    "from grid_environment_utils import * \n",
    "from testing_utils import * \n",
    "import random\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import logging\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import csv\n",
    "import torch\n",
    "from wonderwords import RandomWord\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import permutations\n",
    "import logging\n",
    "from random import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import math\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b275136-35f1-4b43-9070-3eb14e27164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_script(name_or_path='spatial_model', \n",
    "                       num_epochs=3,\n",
    "                       output_dir='./clm_script',\n",
    "                       save_steps=100,\n",
    "                       lr=5e-05):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    ! python ./run_clm.py \\\n",
    "        --model_name_or_path {name_or_path} \\\n",
    "        --train_file {os.path.join(output_dir, 'train.txt')} \\\n",
    "        --validation_file {os.path.join(output_dir, 'train.txt')} \\\n",
    "        --per_device_train_batch_size 1 \\\n",
    "        --per_device_eval_batch_size 1 \\\n",
    "        --do_train \\\n",
    "        --do_eval \\\n",
    "        --output_dir {output_dir} \\\n",
    "        --overwrite_output_dir \\\n",
    "        --num_train_epochs {num_epochs} \\\n",
    "        --save_strategy 'steps' \\\n",
    "        --save_steps {save_steps} \\\n",
    "        --learning_rate {lr} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ed38f-d6b0-443b-ab78-e94cbd3e07e5",
   "metadata": {},
   "source": [
    "#### Train base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516da01f-90f9-4f0e-8cc8-a04198c87d68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_strs = []\n",
    "for i in range(1000):\n",
    "    nouns = [r.word(include_parts_of_speech=[\"nouns\"]).replace(\" \", \"_\") for _ in range(9)]\n",
    "    grid = create_unique_random_grid(nouns)\n",
    "    training_strs.extend(get_all_paths_for_grid(grid))\n",
    "\n",
    "print(f\"{len(training_strs)} shortest paths on arbitrary grids generated for pre-training.\")\n",
    "\n",
    "!mkdir base_model\n",
    "text_file = open(\"base_model/train.txt\", \"w\")\n",
    "n = text_file.write('\\n'.join(training_strs))\n",
    "text_file.close()\n",
    "\n",
    "train_model_script(name_or_path='gpt2', output_dir='base_model', num_epochs=5, save_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8023c6d-8180-48c3-ab08-5f73655c74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r base_model/* base_model_backup/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8424d28",
   "metadata": {},
   "source": [
    "#### Test generative replay\n",
    "\n",
    "Let's first create training data for 5 environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d2d1e-ab29-4f22-935e-fede85bef7e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_on_env(training_strs, testing_strs, eps=10, lr=5e-05, num_train=100, env=0, base_model='base_model', generated_strs=None):\n",
    "    !rm -rf spatial_model_{env}\n",
    "    !mkdir spatial_model_{env}\n",
    "    \n",
    "    text_file = open(f\"spatial_model_{env}/train.txt\", \"w\")\n",
    "    \n",
    "    # There are 100 training sequence so by default use them all\n",
    "    list_to_write = training_strs[env][0:num_train]\n",
    "    \n",
    "    if generated_strs is not None:\n",
    "        list_to_write.extend(generated_strs)\n",
    "\n",
    "    # We oversample the list of training sequences \n",
    "    # This avoids overfitting to a particular sequence order\n",
    "    list_to_write = np.random.choice(list_to_write, 1000).tolist()\n",
    "    n = text_file.write('\\n'.join(list_to_write))\n",
    "    text_file.close()\n",
    "\n",
    "    text_file = open(f\"spatial_model_{env}/test.txt\", \"w\")\n",
    "    n = text_file.write('\\n'.join(testing_strs[env]))\n",
    "    text_file.close()\n",
    "    \n",
    "    train_model_script(name_or_path=base_model, \n",
    "                       output_dir=f'spatial_model_{env}', \n",
    "                       num_epochs=eps, \n",
    "                       save_steps=2000,\n",
    "                       lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3698e1f8-8c87-4c13-9d74-8d0f869ed4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generative_replay(model, num=100, temperature=1):\n",
    "    examples = []\n",
    "    while len(examples) < num:\n",
    "        out = model.continue_input(\"FROM:\", \n",
    "                                   do_sample=True,\n",
    "                                   temperature=temperature)\n",
    "        # Leave out the last sequence as it stopped midway through\n",
    "        examples.extend(out.split('\\n')[:-1])\n",
    "    shuffle(examples)\n",
    "    return examples\n",
    "\n",
    "def experience_replay(i, train_size=100, sample_size=10):\n",
    "    # Get sample_size items from the first train_size items of each previous environment\n",
    "    train_list = [training_strs[j][0:train_size] for j in range(0,i)]\n",
    "    # Flatten this list\n",
    "    train_list = [x for xs in train_list for x in xs]\n",
    "    return random.choices(train_list, k=sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c290f6-b60a-4e68-9834-9b395403ea28",
   "metadata": {},
   "source": [
    "#### Test different parameters\n",
    "\n",
    "* Vary the temperature of sampling\n",
    "* Vary the number of samples\n",
    "* Vary the amount of training per new environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7128f-dd62-49cd-999f-31a18d189472",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "word_freq_results = []\n",
    "\n",
    "num_env = 5\n",
    "num_trials = 3\n",
    "\n",
    "\n",
    "params_to_test = [{'sample_size': 50, 'train_size': 100, 'temp': -1},\n",
    "                  {'sample_size': 50, 'train_size': 100, 'temp': 0.3},\n",
    "                  {'sample_size': 50, 'train_size': 100, 'temp': 0.9},\n",
    "                  {'sample_size': 50, 'train_size': 100, 'temp': 1.5},\n",
    "                  {'sample_size': 50, 'train_size': 100, 'temp': 2.1},\n",
    "                  {'sample_size': 0, 'train_size': 100, 'temp': 1.2},\n",
    "                  {'sample_size': 10, 'train_size': 100, 'temp': 1.2},\n",
    "                  {'sample_size': 50, 'train_size': 100, 'temp': 1.2},\n",
    "                  {'sample_size': 100, 'train_size': 100, 'temp': 1.2},\n",
    "                  {'sample_size': 200, 'train_size': 100, 'temp': 1.2}]\n",
    "\n",
    "for trial_num in range(num_trials):\n",
    "    \n",
    "    training_strs, testing_strs = prepare_data(default=True)\n",
    "\n",
    "    for params in params_to_test:\n",
    "        train_size = params['train_size']\n",
    "        sample_size = params['sample_size']\n",
    "        temp = params['temp']\n",
    "                \n",
    "        for i in range(num_env):\n",
    "            if temp == -1:\n",
    "                generated_strs = experience_replay(i, train_size=train_size, sample_size=sample_size) if i > 0 else []\n",
    "            else:\n",
    "                generated_strs = generative_replay(GPT(base_model=f'spatial_model_{i-1}'), num=sample_size, temperature=temp) if i > 0 else []\n",
    "                \n",
    "            print(generated_strs)\n",
    "            train_on_env(training_strs, \n",
    "                         testing_strs, \n",
    "                         eps=20, \n",
    "                         lr=5e-05,\n",
    "                         num_train=train_size, \n",
    "                         env=i, \n",
    "                         base_model='base_model_b8' if i == 0 else f'spatial_model_{i-1}', \n",
    "                         generated_strs=generated_strs)\n",
    "            \n",
    "            # Save the data from generative / experience replay, and unique locations, for analysis\n",
    "            locs = get_unique_locations(generated_strs)\n",
    "            word_freq_results.append({'model': i, \n",
    "                                      'locs': locs, \n",
    "                                      'temp': temp, \n",
    "                                      'train_size': train_size, \n",
    "                                      \"sample_size\": sample_size, \n",
    "                                      \"seqs\": generated_strs, \n",
    "                                      \"training_strs\": training_strs, \n",
    "                                      \"testing_strs\": testing_strs})\n",
    "            with open('word_freq_results_imagined.pkl', 'wb') as file:\n",
    "                pickle.dump(word_freq_results, file)\n",
    "            \n",
    "            # Test on all environments\n",
    "            model = GPT(base_model=f'spatial_model_{i}')\n",
    "            for j in range(num_env):\n",
    "                if j<=i:\n",
    "                    with open(f\"spatial_model_{j}/test.txt\", 'r') as file:\n",
    "                        test_data = [line.strip() for line in file]\n",
    "                        print(test_data)\n",
    "                    accuracy = test_accuracy(model, test_data)\n",
    "                    results.append(['next_node', i, j, accuracy, temp, train_size, sample_size])\n",
    "                    accuracy = shortest_path_accuracy(model, \n",
    "                                                      test_data_subset(test_data, training_strs[j][:train_size]), \n",
    "                                                      training_strs[j] + testing_strs[j])\n",
    "                    results.append(['shortest_path', i, j, accuracy, temp, train_size, sample_size])\n",
    "    \n",
    "            # Save at intervals in case code errors before end\n",
    "            with open('replay_results_imagined.csv', 'w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['test_type', 'trained_on', 'tested_on', 'accuracy', 'temp', 'train_size', 'sample_size'])\n",
    "                writer.writerows(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cbcb7d-6fce-48d4-90b9-e638df885f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('replay_results_imagined_1102_final.csv')\n",
    "df = df[df[\"temp\"] == 1.2]\n",
    "df = df[df['test_type'] == 'next_node']\n",
    "var_to_test = 'sample_size'\n",
    "\n",
    "# Group by var_to_test, 'Trained_On', and 'Tested_On', and calculate mean and SEM\n",
    "grouped = df.groupby([var_to_test, 'trained_on', 'tested_on'])\n",
    "mean_df = grouped['accuracy'].mean().reset_index()\n",
    "sem_df = grouped['accuracy'].sem().reset_index()\n",
    "\n",
    "#sem_df['accuracy'] = 0 # ignore as there is just one trial\n",
    "\n",
    "vals = mean_df[var_to_test].unique()\n",
    "num_env = df['trained_on'].nunique()\n",
    "\n",
    "# Create a figure with subplots\n",
    "vals_subset = [0, 10, 50, 100]\n",
    "fig, axes = plt.subplots(len(vals_subset), 1, figsize=(6, 9), sharex=True)\n",
    "\n",
    "# Iterate over each sample size and create a subplot\n",
    "for i, val in enumerate(vals_subset):\n",
    "    df_sample_mean = mean_df[mean_df[var_to_test] == val]\n",
    "    df_sample_sem = sem_df[sem_df[var_to_test] == val]\n",
    "\n",
    "    for tested_on in range(num_env):\n",
    "        # Filter the mean and SEM dataframes for the specific 'Tested_On' value\n",
    "        means = df_sample_mean[df_sample_mean['tested_on'] == tested_on]['accuracy']\n",
    "        sems = df_sample_sem[df_sample_sem['tested_on'] == tested_on]['accuracy']\n",
    "        trained_on_values = df_sample_mean[df_sample_mean['tested_on'] == tested_on]['trained_on']\n",
    "        \n",
    "        # Plot error bars\n",
    "        axes[i].errorbar(trained_on_values, means, yerr=sems, label=f'Tested on Env {tested_on}', marker='o')\n",
    "\n",
    "    letter = string.ascii_lowercase[i]\n",
    "    axes[i].set_title(f'{letter}) {val} self-generated samples')\n",
    "    axes[i].set_ylabel('Accuracy')\n",
    "    axes[i].set_ylim((0,1.05))\n",
    "    axes[i].legend()\n",
    "\n",
    "# Set common labels and title\n",
    "axes[-1].set_xticks(range(num_env))\n",
    "axes[-1].set_xticklabels([F'Env. {n}' for n in range(num_env)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Sample effect.png', dpi=500)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194c476-d762-4322-b1fc-0bc02c0fed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('replay_results_imagined_1102_final.csv')\n",
    "df = df[df[\"sample_size\"] == 50][df[\"train_size\"] == 100]\n",
    "df = df[df['test_type'] == 'next_node']\n",
    "var_to_test = 'temp'\n",
    "\n",
    "# Group by var_to_test, 'Trained_On', and 'Tested_On', and calculate mean and SEM\n",
    "grouped = df.groupby([var_to_test, 'trained_on', 'tested_on'])\n",
    "mean_df = grouped['accuracy'].mean().reset_index()\n",
    "sem_df = grouped['accuracy'].sem().reset_index()\n",
    "\n",
    "#sem_df['accuracy'] = 0 # ignore as there is just one trial\n",
    "\n",
    "vals = mean_df[var_to_test].unique()\n",
    "num_env = df['trained_on'].nunique()\n",
    "\n",
    "# Create a figure with subplots\n",
    "vals_subset = [-1, 0.3, 0.9, 1.5, 2.1]\n",
    "fig, axes = plt.subplots(len(vals_subset), 1, figsize=(6, 12), sharex=True)\n",
    "\n",
    "# Iterate over each sample size and create a subplot\n",
    "for i, val in enumerate(vals_subset):\n",
    "    df_sample_mean = mean_df[mean_df[var_to_test] == val]\n",
    "    df_sample_sem = sem_df[sem_df[var_to_test] == val]\n",
    "\n",
    "    for tested_on in range(num_env):\n",
    "        # Filter the mean and SEM dataframes for the specific 'Tested_On' value\n",
    "        means = df_sample_mean[df_sample_mean['tested_on'] == tested_on]['accuracy']\n",
    "        sems = df_sample_sem[df_sample_sem['tested_on'] == tested_on]['accuracy']\n",
    "        trained_on_values = df_sample_mean[df_sample_mean['tested_on'] == tested_on]['trained_on']\n",
    "        \n",
    "        # Plot error bars\n",
    "        axes[i].errorbar(trained_on_values, means, yerr=sems, label=f'Tested on Env {tested_on}', marker='o')\n",
    "\n",
    "    letter = string.ascii_lowercase[i]\n",
    "    axes[i].set_title(f'{letter}) Temperature of {val}' if val > 0 else 'a) Experience replay')\n",
    "    axes[i].set_ylabel('Accuracy')\n",
    "    axes[i].set_ylim((0,1.05))\n",
    "    axes[i].set_xlabel('Training stage')\n",
    "    axes[i].legend()\n",
    "\n",
    "# Set common labels and title\n",
    "axes[-1].set_xticks(range(num_env))\n",
    "axes[-1].set_xticklabels([F'Env. {n}' for n in range(num_env)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Temp effect.png', dpi=500)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07025706-8408-4a76-8910-de9377ec31e3",
   "metadata": {},
   "source": [
    "#### Plot aggregated forgetting rate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18c748-5909-44b6-b9c3-33aeeba6eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('replay_results_imagined_1102_final.csv')\n",
    "df = df[df[\"sample_size\"] == 50][df[\"train_size\"] == 100]\n",
    "df = df[df['test_type'] == 'next_node']\n",
    "df = df[df['tested_on'] <= df['trained_on']]\n",
    "\n",
    "# Group by temperature, trained_on, tested_on to calculate mean accuracy\n",
    "mean_accuracy_df = df.groupby(['temp', 'trained_on', 'tested_on'])['accuracy'].mean().reset_index()\n",
    "\n",
    "# Sort to ensure calculation is done in sequence\n",
    "mean_accuracy_df.sort_values(by=['temp', 'tested_on', 'trained_on'], inplace=True)\n",
    "\n",
    "# Calculate the decrease in accuracy for each subsequent stage\n",
    "mean_accuracy_df['next_accuracy'] = mean_accuracy_df.groupby(['temp', 'tested_on'])['accuracy'].shift(-1)\n",
    "mean_accuracy_df['decrease'] = mean_accuracy_df['next_accuracy'] - mean_accuracy_df['accuracy']\n",
    "\n",
    "# Drop the last stage for each temp and tested_on since it has no subsequent stage to compare\n",
    "mean_accuracy_df.dropna(subset=['decrease'], inplace=True)\n",
    "\n",
    "# Group by temp to calculate mean rate of forgetting\n",
    "mean_forgetting_rate = mean_accuracy_df.groupby('temp')['decrease'].mean().reset_index()\n",
    "\n",
    "# Filter out the rows where 'temp' is -1 before plotting\n",
    "mean_forgetting_rate_filtered = mean_forgetting_rate[mean_forgetting_rate['temp'] != -1]\n",
    "\n",
    "# Continue with plotting using the filtered DataFrame\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.plot(mean_forgetting_rate_filtered['temp'], mean_forgetting_rate_filtered['decrease'], marker='o', linestyle='-', color='blue')  # Plotting with temperature != -1\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Mean accuracy change')\n",
    "plt.savefig('mean_acc_change_temp.png', bbox_inches='tight', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f13e8-774c-4d53-801e-82dcc2f06e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('replay_results_imagined_1102_final.csv')\n",
    "df = df[df['train_size'] == 100][df['temp'] == 1.2]\n",
    "df = df[df['test_type'] == 'next_node']\n",
    "df = df[df['tested_on'] <= df['trained_on']]\n",
    "\n",
    "# Initialize a DataFrame to store the rate of forgetting for each sample size\n",
    "rates_of_forgetting = []\n",
    "\n",
    "# Get unique sample sizes\n",
    "sample_sizes = df['sample_size'].unique()\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    df_sample = df[df['sample_size'] == sample_size]\n",
    "    \n",
    "    # Group by 'trained_on' and 'tested_on' to calculate mean accuracy\n",
    "    grouped = df_sample.groupby(['trained_on', 'tested_on'])['accuracy'].mean().reset_index()\n",
    "    \n",
    "    # Calculate rate of forgetting\n",
    "    grouped['next_accuracy'] = grouped.groupby('tested_on')['accuracy'].shift(-1)\n",
    "    grouped['decrease'] = grouped['accuracy'] - grouped['next_accuracy']\n",
    "    grouped.dropna(subset=['decrease'], inplace=True)\n",
    "    \n",
    "    # Average the decrease for this sample size\n",
    "    mean_decrease = grouped['decrease'].mean()\n",
    "    rates_of_forgetting.append((sample_size, mean_decrease))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "rates_df = pd.DataFrame(rates_of_forgetting, columns=['sample_size', 'mean_rate_of_forgetting'])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.plot(rates_df['sample_size'], -rates_df['mean_rate_of_forgetting'], marker='o', linestyle='-', color='blue')  # Negative sign to show decrease as positive values\n",
    "plt.xlabel('Sample size')\n",
    "plt.yticks([-0.4, -0.3, -0.2, -0.1, 0])\n",
    "plt.ylabel('Mean accuracy change')\n",
    "plt.savefig('mean_acc_change_sample.png', bbox_inches='tight', dpi=500)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
